{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem1: Improving Your Digit Recognition Model From Assignment 1"
      ],
      "metadata": {
        "id": "tVI0jcU1AGjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aJgxJL3EC3xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow_probability as tfp\n",
        "import copy"
      ],
      "metadata": {
        "id": "AnQq2wSksoqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/German_digits.csv')"
      ],
      "metadata": {
        "id": "VOaj1VUDC_zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "arIhWD6cDAzL",
        "outputId": "52d4a15b-b95f-4e94-f1cb-745981ca3701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   1  2  3  4  5  6  7  8  9  10  ...  1592  1593  1594  1595  1596  1597  \\\n",
              "0  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n",
              "1  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n",
              "2  0  0  0  0  0  0  0  0  0  80  ...     0     0     0     0     0     0   \n",
              "3  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n",
              "4  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n",
              "\n",
              "   1598  1599  1600  1601  \n",
              "0     0     0     0     3  \n",
              "1     0     0     0     5  \n",
              "2     0     0     0     7  \n",
              "3     0     0     0     0  \n",
              "4     0     0     0     6  \n",
              "\n",
              "[5 rows x 1601 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90974ba9-44e2-4cbf-9315-cfc52cd006b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>1592</th>\n",
              "      <th>1593</th>\n",
              "      <th>1594</th>\n",
              "      <th>1595</th>\n",
              "      <th>1596</th>\n",
              "      <th>1597</th>\n",
              "      <th>1598</th>\n",
              "      <th>1599</th>\n",
              "      <th>1600</th>\n",
              "      <th>1601</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1601 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90974ba9-44e2-4cbf-9315-cfc52cd006b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90974ba9-44e2-4cbf-9315-cfc52cd006b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90974ba9-44e2-4cbf-9315-cfc52cd006b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a7b91829-32b8-41d2-ae2c-85bee8b0a0d2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7b91829-32b8-41d2-ae2c-85bee8b0a0d2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a7b91829-32b8-41d2-ae2c-85bee8b0a0d2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BL67kt7DIJc",
        "outputId": "108a9c1a-8815-4074-986a-10fbeaa33c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4426, 1601)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The first 1600 columns are the pixel values, and the last column is the label\n",
        "X = df.iloc[:, :-1].values  # features (pixel values)\n",
        "Y = df.iloc[:, -1].values   # labels (digits 0-9)"
      ],
      "metadata": {
        "id": "HkpB1KHTDJCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X / 255.0"
      ],
      "metadata": {
        "id": "kCGpRBDODMgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly split  dataset into train and test. Used  train_test_split from sklearn package\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "p2ohKaYfDPaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding the labels\n",
        "Y_train_onehot = pd.get_dummies(Y_train).values\n",
        "Y_val_onehot = pd.get_dummies(Y_val).values"
      ],
      "metadata": {
        "id": "uAhhAqAKDS-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_onehot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0m_y7dbDXjh",
        "outputId": "0ca5e225-7b44-42d6-dc06-bdb354da798b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3540, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposed the feature matrix and the one-hot encoded labels\n",
        "X_train_T = X_train.T\n",
        "X_val_T = X_val.T\n",
        "Y_train_onehot_T = Y_train_onehot.T\n",
        "Y_val_onehot_T = Y_val_onehot.T"
      ],
      "metadata": {
        "id": "Iw4eVjw4x4Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train X shape:\", X_train_T.shape)\n",
        "print(\"Train Y shape:\", Y_train_onehot_T.shape)\n",
        "print(\"Validation X shape:\", X_val_T.shape)\n",
        "print(\"Validation Y shape:\", Y_val_onehot_T.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsLFgot7x5Lj",
        "outputId": "d01ac665-86a8-459d-b7b3-471bc713da67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X shape: (1600, 3540)\n",
            "Train Y shape: (10, 3540)\n",
            "Validation X shape: (1600, 886)\n",
            "Validation Y shape: (10, 886)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X_train_T:\", X_train_T.shape)  # Shape - (num_features, num_samples)\n",
        "print(\"Shape of Y_train:\", Y_train.shape)      # Shape - (num_classes, num_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxZ4lmWmx8Lt",
        "outputId": "f985f4b6-247c-4760-f43d-1bdd2afe4c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_T: (1600, 3540)\n",
            "Shape of Y_train: (3540,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(nx, nh):\n",
        "  parameters={}\n",
        "  # Initialize weights and biases for the first layer (input to the first hidden layer)\n",
        "  parameters['W1'] = tf.Variable(tf.random.uniform(shape=(nh[0], nx), minval=-0.01, maxval=0.01))\n",
        "  #biases are typically initialized to zeros\n",
        "  parameters['b1'] = tf.Variable(tf.zeros((nh[0],1)))\n",
        "\n",
        "\n",
        "  for i in range(1, len(nh)):\n",
        "    parameters['W'+str(i+1)]=tf.Variable(tf.random.uniform(shape=(nh[i],nh[i-1]), minval=-0.01, maxval=0.01))\n",
        "    parameters['b'+str(i+1)]=tf.Variable(tf.zeros((nh[i],1)))\n",
        "\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n",
        "parameters= initialize_parameters(1600, [512,256,128,10])\n",
        "for key, value in parameters.items():\n",
        "  print(key, value.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ4Y408QyAR0",
        "outputId": "2a101f92-470a-453f-cb37-e701138c755d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 (512, 1600)\n",
            "b1 (512, 1)\n",
            "W2 (256, 512)\n",
            "b2 (256, 1)\n",
            "W3 (128, 256)\n",
            "b3 (128, 1)\n",
            "W4 (10, 128)\n",
            "b4 (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)  # Cast input to float32\n",
        "\n",
        "    n_layers = len(parameters) // 2  # Number of layers (W and b pairs)\n",
        "\n",
        "    # First layer (input to first hidden layer)\n",
        "    Z = tf.matmul(parameters['W1'], X) + parameters['b1']\n",
        "    A = tf.nn.relu(Z)\n",
        "\n",
        "    # Loop through remaining hidden layers\n",
        "    for i in range(1, n_layers - 1):\n",
        "        Z = tf.matmul(parameters['W' + str(i + 1)], A) + parameters['b' + str(i + 1)]\n",
        "        A = tf.nn.relu(Z)\n",
        "\n",
        "    # Final layer (no ReLU, apply softmax for classification)\n",
        "    Z = tf.matmul(parameters['W' + str(n_layers)], A) + parameters['b' + str(n_layers)]\n",
        "\n",
        "\n",
        "\n",
        "    A = tf.nn.softmax(Z, axis=0)  # Softmax for multi-class classification\n",
        "\n",
        "\n",
        "\n",
        "    return A\n"
      ],
      "metadata": {
        "id": "vi1nwAJYyDJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unit testing forward_pass\n",
        "parameters= initialize_parameters(1600, [512,256,128,10])\n",
        "\n",
        "Yhat=forward_pass(parameters,X_train_T)\n",
        "print(Yhat.shape)"
      ],
      "metadata": {
        "id": "B7ZulIhlyIKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if softmax is working correctly by verifying that the probabilities sum to 1\n",
        "#Yhat=forward_pass(parameters,X_train_T)\n",
        "print(\"Sum of probabilities for first sample:\", np.sum(Yhat[:, 0]))  # Should be 1.0"
      ],
      "metadata": {
        "id": "m2yZzuptyMzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_val = forward_pass(parameters, X_val_T)\n",
        "#print(\"Shape of model output (Validation):\", yhat_val.shape)"
      ],
      "metadata": {
        "id": "Yfd0Xwd7yQKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical cross entropy loss function\n",
        "def compute_loss(Y, Yhat):\n",
        "    # Use TensorFlow's built-in categorical cross-entropy loss for stability\n",
        "    #loss_per_sample = tf.keras.losses.categorical_crossentropy(Y, Yhat, from_logits=False)\n",
        "    # Compute categorical cross-entropy loss for multi-class classification\n",
        "    loss_per_sample = -tf.reduce_sum(Y * tf.math.log(Yhat + 1e-8), axis=0)  # Add small value to avoid log(0)\n",
        "    return tf.reduce_mean(loss_per_sample)  # Take the mean across all samples"
      ],
      "metadata": {
        "id": "cZ-pR0TeyTPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#  testing loss\n",
        "# Y_train_onehot_T is the actual one-hot encoded labels\n",
        "# Yhat is the predicted output from forward_pass\n",
        "y = Y_train_onehot_T  # Actual one-hot encoded labels\n",
        "yhat = forward_pass(parameters, X_train_T)\n",
        "loss = compute_loss(y, yhat)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "VKAQpCLNyYdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(loss, parameters, tape):\n",
        "    # Extract the list of variables from the parameters dictionary\n",
        "    variables = list(parameters.values())\n",
        "    # Compute the gradients with respect to the loss\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "2VyTmxV_ybfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    # Zip through both parameters and gradients to apply updates\n",
        "    for (key, param), grad in zip(parameters.items(), gradients):\n",
        "        parameters[key].assign_sub(learning_rate * grad)\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "8a5zbhL8yf7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improve  Digit Recognition Model:\n",
        "\n",
        "More specifically, we will add the following components.\n",
        "\n",
        " create_nn method with  Mini-batch gradient descent\n",
        " - create_nn method should accept an additional parameter (batch_size). Then instead of\n",
        "updating parameters per entire data, we will create random batches of (batch_size) samples from training\n",
        "data, run forward and backward passes, and update parameters per each batch. Then to get the train_loss\n",
        "for each epoch, we should keep track and average train_losses over batches.\n",
        "1- Retrain the model using mini-batch gradient descent  implemented above and interpret the learning\n",
        "curves.\n",
        "\n",
        "• How do training and validation losses compare to the model without minibatch gradient\n",
        "descent?"
      ],
      "metadata": {
        "id": "mEykKL7Gvjrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Referred google\n",
        "# Function to create random mini-batches\n",
        "def create_mini_batches(X, Y, batch_size):\n",
        "    m = X.shape[1]  # Number of training examples\n",
        "    permutation = np.random.permutation(m)\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation]\n",
        "\n",
        "    mini_batches = []\n",
        "\n",
        "    num_batches = m // batch_size\n",
        "\n",
        "    for k in range(num_batches):\n",
        "        mini_batch_X = shuffled_X[:, k * batch_size:(k + 1) * batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * batch_size:(k + 1) * batch_size]\n",
        "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
        "\n",
        "    # To handle the last mini-batch (if m is not divisible by batch_size)\n",
        "    if m % batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_batches * batch_size:]\n",
        "        mini_batch_Y = shuffled_Y[:, num_batches * batch_size:]\n",
        "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
        "\n",
        "    return mini_batches"
      ],
      "metadata": {
        "id": "DN9pAubGhqea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Referred google\n",
        "\n",
        "\n",
        "\n",
        "def create_nn(train_X, train_Y, val_X, val_Y, nh, learning_rate, iterations, batch_size):\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(train_X.shape[0], nh)\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        mini_batches = create_mini_batches(train_X, train_Y, batch_size)\n",
        "        epoch_train_losses = []\n",
        "\n",
        "        for (mini_batch_X, mini_batch_Y) in mini_batches:\n",
        "            # Compute train loss and gradients for each mini-batch\n",
        "            with tf.GradientTape() as tape:\n",
        "                yhat_train = forward_pass(parameters, mini_batch_X)\n",
        "                train_loss = compute_loss(mini_batch_Y, yhat_train)\n",
        "\n",
        "            # Record mini-batch loss\n",
        "            epoch_train_losses.append(train_loss.numpy())\n",
        "\n",
        "            # Compute gradients and update parameters\n",
        "            gradients = compute_gradients(train_loss, parameters, tape)\n",
        "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "        # Average the loss over all mini-batches for the epoch\n",
        "        avg_train_loss = np.mean(epoch_train_losses)\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "\n",
        "        # Compute validation loss after each epoch\n",
        "        yhat_val = forward_pass(parameters, val_X)\n",
        "        val_loss = compute_loss(val_Y, yhat_val)\n",
        "        val_loss_history.append(val_loss.numpy())\n",
        "\n",
        "        print(f\"Iteration {i}: Avg Train Loss: {avg_train_loss}, Validation Loss: {val_loss}\")\n",
        "\n",
        "    return parameters, train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "IYGtdesLrk2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, train_history, val_history=create_nn(X_train_T, Y_train_onehot_T, X_val_T, Y_val_onehot_T, [512,256,128,10], 0.01, 500,32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcjoZPsPs-oD",
        "outputId": "38b0f24a-c946-45f3-ec33-c5898e4260f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Avg Train Loss: 2.302677631378174, Validation Loss: 2.3025453090667725\n",
            "Iteration 1: Avg Train Loss: 2.3025872707366943, Validation Loss: 2.302525758743286\n",
            "Iteration 2: Avg Train Loss: 2.3025147914886475, Validation Loss: 2.3025174140930176\n",
            "Iteration 3: Avg Train Loss: 2.3024823665618896, Validation Loss: 2.302521228790283\n",
            "Iteration 4: Avg Train Loss: 2.302417755126953, Validation Loss: 2.302527666091919\n",
            "Iteration 5: Avg Train Loss: 2.3023877143859863, Validation Loss: 2.302546262741089\n",
            "Iteration 6: Avg Train Loss: 2.302381992340088, Validation Loss: 2.302532911300659\n",
            "Iteration 7: Avg Train Loss: 2.302340507507324, Validation Loss: 2.302536964416504\n",
            "Iteration 8: Avg Train Loss: 2.302338123321533, Validation Loss: 2.302546739578247\n",
            "Iteration 9: Avg Train Loss: 2.302319049835205, Validation Loss: 2.3025472164154053\n",
            "Iteration 10: Avg Train Loss: 2.3023273944854736, Validation Loss: 2.30255389213562\n",
            "Iteration 11: Avg Train Loss: 2.30226993560791, Validation Loss: 2.302560806274414\n",
            "Iteration 12: Avg Train Loss: 2.3022663593292236, Validation Loss: 2.302572250366211\n",
            "Iteration 13: Avg Train Loss: 2.3022961616516113, Validation Loss: 2.302586317062378\n",
            "Iteration 14: Avg Train Loss: 2.302265167236328, Validation Loss: 2.3025882244110107\n",
            "Iteration 15: Avg Train Loss: 2.3022725582122803, Validation Loss: 2.3026013374328613\n",
            "Iteration 16: Avg Train Loss: 2.3022491931915283, Validation Loss: 2.302614688873291\n",
            "Iteration 17: Avg Train Loss: 2.3022663593292236, Validation Loss: 2.3026204109191895\n",
            "Iteration 18: Avg Train Loss: 2.3022329807281494, Validation Loss: 2.302645683288574\n",
            "Iteration 19: Avg Train Loss: 2.3022258281707764, Validation Loss: 2.302643060684204\n",
            "Iteration 20: Avg Train Loss: 2.302258014678955, Validation Loss: 2.302647352218628\n",
            "Iteration 21: Avg Train Loss: 2.302250385284424, Validation Loss: 2.302640914916992\n",
            "Iteration 22: Avg Train Loss: 2.3022828102111816, Validation Loss: 2.302652597427368\n",
            "Iteration 23: Avg Train Loss: 2.3022427558898926, Validation Loss: 2.302659511566162\n",
            "Iteration 24: Avg Train Loss: 2.302241086959839, Validation Loss: 2.302671194076538\n",
            "Iteration 25: Avg Train Loss: 2.302265167236328, Validation Loss: 2.302675724029541\n",
            "Iteration 26: Avg Train Loss: 2.3022279739379883, Validation Loss: 2.3026843070983887\n",
            "Iteration 27: Avg Train Loss: 2.3022220134735107, Validation Loss: 2.302673101425171\n",
            "Iteration 28: Avg Train Loss: 2.3022713661193848, Validation Loss: 2.302661418914795\n",
            "Iteration 29: Avg Train Loss: 2.3022725582122803, Validation Loss: 2.3026721477508545\n",
            "Iteration 30: Avg Train Loss: 2.3022449016571045, Validation Loss: 2.3026623725891113\n",
            "Iteration 31: Avg Train Loss: 2.30224609375, Validation Loss: 2.302671194076538\n",
            "Iteration 32: Avg Train Loss: 2.3022661209106445, Validation Loss: 2.30265736579895\n",
            "Iteration 33: Avg Train Loss: 2.30228328704834, Validation Loss: 2.3026623725891113\n",
            "Iteration 34: Avg Train Loss: 2.302218437194824, Validation Loss: 2.3026621341705322\n",
            "Iteration 35: Avg Train Loss: 2.3022549152374268, Validation Loss: 2.3026621341705322\n",
            "Iteration 36: Avg Train Loss: 2.3022241592407227, Validation Loss: 2.3026652336120605\n",
            "Iteration 37: Avg Train Loss: 2.3022265434265137, Validation Loss: 2.302665948867798\n",
            "Iteration 38: Avg Train Loss: 2.3022637367248535, Validation Loss: 2.3026607036590576\n",
            "Iteration 39: Avg Train Loss: 2.302231550216675, Validation Loss: 2.3026769161224365\n",
            "Iteration 40: Avg Train Loss: 2.302276134490967, Validation Loss: 2.3026785850524902\n",
            "Iteration 41: Avg Train Loss: 2.3021914958953857, Validation Loss: 2.3026838302612305\n",
            "Iteration 42: Avg Train Loss: 2.302236795425415, Validation Loss: 2.302676200866699\n",
            "Iteration 43: Avg Train Loss: 2.3022429943084717, Validation Loss: 2.3026881217956543\n",
            "Iteration 44: Avg Train Loss: 2.302253007888794, Validation Loss: 2.3027071952819824\n",
            "Iteration 45: Avg Train Loss: 2.302263021469116, Validation Loss: 2.3027117252349854\n",
            "Iteration 46: Avg Train Loss: 2.302194356918335, Validation Loss: 2.302710771560669\n",
            "Iteration 47: Avg Train Loss: 2.3022475242614746, Validation Loss: 2.3027002811431885\n",
            "Iteration 48: Avg Train Loss: 2.30220890045166, Validation Loss: 2.302706003189087\n",
            "Iteration 49: Avg Train Loss: 2.3021950721740723, Validation Loss: 2.302710771560669\n",
            "Iteration 50: Avg Train Loss: 2.302248477935791, Validation Loss: 2.3026931285858154\n",
            "Iteration 51: Avg Train Loss: 2.3022263050079346, Validation Loss: 2.3026959896087646\n",
            "Iteration 52: Avg Train Loss: 2.3022079467773438, Validation Loss: 2.302706480026245\n",
            "Iteration 53: Avg Train Loss: 2.30222225189209, Validation Loss: 2.3027150630950928\n",
            "Iteration 54: Avg Train Loss: 2.302229642868042, Validation Loss: 2.302706480026245\n",
            "Iteration 55: Avg Train Loss: 2.3022232055664062, Validation Loss: 2.3027238845825195\n",
            "Iteration 56: Avg Train Loss: 2.302191734313965, Validation Loss: 2.3027195930480957\n",
            "Iteration 57: Avg Train Loss: 2.302227735519409, Validation Loss: 2.3027234077453613\n",
            "Iteration 58: Avg Train Loss: 2.302227020263672, Validation Loss: 2.3027405738830566\n",
            "Iteration 59: Avg Train Loss: 2.3022265434265137, Validation Loss: 2.3027398586273193\n",
            "Iteration 60: Avg Train Loss: 2.302229881286621, Validation Loss: 2.302731513977051\n",
            "Iteration 61: Avg Train Loss: 2.3022208213806152, Validation Loss: 2.3027443885803223\n",
            "Iteration 62: Avg Train Loss: 2.3022382259368896, Validation Loss: 2.3027398586273193\n",
            "Iteration 63: Avg Train Loss: 2.3021962642669678, Validation Loss: 2.3027355670928955\n",
            "Iteration 64: Avg Train Loss: 2.302229881286621, Validation Loss: 2.302738666534424\n",
            "Iteration 65: Avg Train Loss: 2.30222749710083, Validation Loss: 2.302741527557373\n",
            "Iteration 66: Avg Train Loss: 2.3022189140319824, Validation Loss: 2.3027470111846924\n",
            "Iteration 67: Avg Train Loss: 2.302219867706299, Validation Loss: 2.302746057510376\n",
            "Iteration 68: Avg Train Loss: 2.302225112915039, Validation Loss: 2.3027336597442627\n",
            "Iteration 69: Avg Train Loss: 2.302234172821045, Validation Loss: 2.302738904953003\n",
            "Iteration 70: Avg Train Loss: 2.3021974563598633, Validation Loss: 2.3027355670928955\n",
            "Iteration 71: Avg Train Loss: 2.3022356033325195, Validation Loss: 2.3027305603027344\n",
            "Iteration 72: Avg Train Loss: 2.302253484725952, Validation Loss: 2.3027403354644775\n",
            "Iteration 73: Avg Train Loss: 2.3022031784057617, Validation Loss: 2.302750587463379\n",
            "Iteration 74: Avg Train Loss: 2.3021750450134277, Validation Loss: 2.3027255535125732\n",
            "Iteration 75: Avg Train Loss: 2.30220365524292, Validation Loss: 2.302722215652466\n",
            "Iteration 76: Avg Train Loss: 2.3022420406341553, Validation Loss: 2.302718162536621\n",
            "Iteration 77: Avg Train Loss: 2.302184820175171, Validation Loss: 2.3027186393737793\n",
            "Iteration 78: Avg Train Loss: 2.302227258682251, Validation Loss: 2.302706241607666\n",
            "Iteration 79: Avg Train Loss: 2.3022570610046387, Validation Loss: 2.302699089050293\n",
            "Iteration 80: Avg Train Loss: 2.3022093772888184, Validation Loss: 2.302690267562866\n",
            "Iteration 81: Avg Train Loss: 2.302190065383911, Validation Loss: 2.302682399749756\n",
            "Iteration 82: Avg Train Loss: 2.302208185195923, Validation Loss: 2.3026883602142334\n",
            "Iteration 83: Avg Train Loss: 2.302222490310669, Validation Loss: 2.302682638168335\n",
            "Iteration 84: Avg Train Loss: 2.3022005558013916, Validation Loss: 2.3026931285858154\n",
            "Iteration 85: Avg Train Loss: 2.3022122383117676, Validation Loss: 2.302687883377075\n",
            "Iteration 86: Avg Train Loss: 2.3022336959838867, Validation Loss: 2.3026649951934814\n",
            "Iteration 87: Avg Train Loss: 2.3021633625030518, Validation Loss: 2.3026809692382812\n",
            "Iteration 88: Avg Train Loss: 2.3022091388702393, Validation Loss: 2.3026845455169678\n",
            "Iteration 89: Avg Train Loss: 2.3022143840789795, Validation Loss: 2.3026769161224365\n",
            "Iteration 90: Avg Train Loss: 2.302191734313965, Validation Loss: 2.302671432495117\n",
            "Iteration 91: Avg Train Loss: 2.3021864891052246, Validation Loss: 2.3026514053344727\n",
            "Iteration 92: Avg Train Loss: 2.3022172451019287, Validation Loss: 2.3026511669158936\n",
            "Iteration 93: Avg Train Loss: 2.3021936416625977, Validation Loss: 2.3026506900787354\n",
            "Iteration 94: Avg Train Loss: 2.30216121673584, Validation Loss: 2.3026716709136963\n",
            "Iteration 95: Avg Train Loss: 2.3021819591522217, Validation Loss: 2.302671432495117\n",
            "Iteration 96: Avg Train Loss: 2.3021676540374756, Validation Loss: 2.3026633262634277\n",
            "Iteration 97: Avg Train Loss: 2.3021457195281982, Validation Loss: 2.3026764392852783\n",
            "Iteration 98: Avg Train Loss: 2.302185535430908, Validation Loss: 2.302669048309326\n",
            "Iteration 99: Avg Train Loss: 2.3022143840789795, Validation Loss: 2.302668571472168\n",
            "Iteration 100: Avg Train Loss: 2.302194356918335, Validation Loss: 2.302663803100586\n",
            "Iteration 101: Avg Train Loss: 2.3021762371063232, Validation Loss: 2.302666425704956\n",
            "Iteration 102: Avg Train Loss: 2.3021655082702637, Validation Loss: 2.3026537895202637\n",
            "Iteration 103: Avg Train Loss: 2.3021624088287354, Validation Loss: 2.3026552200317383\n",
            "Iteration 104: Avg Train Loss: 2.3021769523620605, Validation Loss: 2.3026463985443115\n",
            "Iteration 105: Avg Train Loss: 2.3021745681762695, Validation Loss: 2.3026394844055176\n",
            "Iteration 106: Avg Train Loss: 2.30220365524292, Validation Loss: 2.3026466369628906\n",
            "Iteration 107: Avg Train Loss: 2.3021745681762695, Validation Loss: 2.302640914916992\n",
            "Iteration 108: Avg Train Loss: 2.302164077758789, Validation Loss: 2.302624225616455\n",
            "Iteration 109: Avg Train Loss: 2.3021576404571533, Validation Loss: 2.3026251792907715\n",
            "Iteration 110: Avg Train Loss: 2.302158832550049, Validation Loss: 2.3026375770568848\n",
            "Iteration 111: Avg Train Loss: 2.302165985107422, Validation Loss: 2.302626609802246\n",
            "Iteration 112: Avg Train Loss: 2.3021798133850098, Validation Loss: 2.3026232719421387\n",
            "Iteration 113: Avg Train Loss: 2.302130937576294, Validation Loss: 2.3026034832000732\n",
            "Iteration 114: Avg Train Loss: 2.302159070968628, Validation Loss: 2.3026270866394043\n",
            "Iteration 115: Avg Train Loss: 2.302140951156616, Validation Loss: 2.3026278018951416\n",
            "Iteration 116: Avg Train Loss: 2.3021154403686523, Validation Loss: 2.302617311477661\n",
            "Iteration 117: Avg Train Loss: 2.302135705947876, Validation Loss: 2.3026130199432373\n",
            "Iteration 118: Avg Train Loss: 2.3021204471588135, Validation Loss: 2.3026251792907715\n",
            "Iteration 119: Avg Train Loss: 2.3021044731140137, Validation Loss: 2.3025999069213867\n",
            "Iteration 120: Avg Train Loss: 2.3021421432495117, Validation Loss: 2.302591562271118\n",
            "Iteration 121: Avg Train Loss: 2.3021106719970703, Validation Loss: 2.302602767944336\n",
            "Iteration 122: Avg Train Loss: 2.302121162414551, Validation Loss: 2.302605152130127\n",
            "Iteration 123: Avg Train Loss: 2.3021528720855713, Validation Loss: 2.3025901317596436\n",
            "Iteration 124: Avg Train Loss: 2.3021080493927, Validation Loss: 2.302581548690796\n",
            "Iteration 125: Avg Train Loss: 2.3021278381347656, Validation Loss: 2.3025779724121094\n",
            "Iteration 126: Avg Train Loss: 2.3020823001861572, Validation Loss: 2.3025848865509033\n",
            "Iteration 127: Avg Train Loss: 2.3020951747894287, Validation Loss: 2.302586078643799\n",
            "Iteration 128: Avg Train Loss: 2.302114963531494, Validation Loss: 2.3025944232940674\n",
            "Iteration 129: Avg Train Loss: 2.302119493484497, Validation Loss: 2.302574872970581\n",
            "Iteration 130: Avg Train Loss: 2.302081346511841, Validation Loss: 2.302555561065674\n",
            "Iteration 131: Avg Train Loss: 2.3020803928375244, Validation Loss: 2.3025457859039307\n",
            "Iteration 132: Avg Train Loss: 2.302030324935913, Validation Loss: 2.302520513534546\n",
            "Iteration 133: Avg Train Loss: 2.302051544189453, Validation Loss: 2.3025150299072266\n",
            "Iteration 134: Avg Train Loss: 2.3020689487457275, Validation Loss: 2.3025143146514893\n",
            "Iteration 135: Avg Train Loss: 2.3020195960998535, Validation Loss: 2.3025107383728027\n",
            "Iteration 136: Avg Train Loss: 2.3020098209381104, Validation Loss: 2.302485942840576\n",
            "Iteration 137: Avg Train Loss: 2.302049398422241, Validation Loss: 2.3024699687957764\n",
            "Iteration 138: Avg Train Loss: 2.3019888401031494, Validation Loss: 2.3024418354034424\n",
            "Iteration 139: Avg Train Loss: 2.3019816875457764, Validation Loss: 2.302436113357544\n",
            "Iteration 140: Avg Train Loss: 2.3019521236419678, Validation Loss: 2.302420139312744\n",
            "Iteration 141: Avg Train Loss: 2.3020222187042236, Validation Loss: 2.3024189472198486\n",
            "Iteration 142: Avg Train Loss: 2.301950454711914, Validation Loss: 2.3023996353149414\n",
            "Iteration 143: Avg Train Loss: 2.3019518852233887, Validation Loss: 2.3024041652679443\n",
            "Iteration 144: Avg Train Loss: 2.301947593688965, Validation Loss: 2.302363157272339\n",
            "Iteration 145: Avg Train Loss: 2.301966428756714, Validation Loss: 2.302356004714966\n",
            "Iteration 146: Avg Train Loss: 2.3019163608551025, Validation Loss: 2.302328586578369\n",
            "Iteration 147: Avg Train Loss: 2.301907539367676, Validation Loss: 2.3023176193237305\n",
            "Iteration 148: Avg Train Loss: 2.3018758296966553, Validation Loss: 2.302311658859253\n",
            "Iteration 149: Avg Train Loss: 2.301853656768799, Validation Loss: 2.302290201187134\n",
            "Iteration 150: Avg Train Loss: 2.3018171787261963, Validation Loss: 2.3022680282592773\n",
            "Iteration 151: Avg Train Loss: 2.301804542541504, Validation Loss: 2.302269220352173\n",
            "Iteration 152: Avg Train Loss: 2.301740884780884, Validation Loss: 2.3022360801696777\n",
            "Iteration 153: Avg Train Loss: 2.301755428314209, Validation Loss: 2.302197217941284\n",
            "Iteration 154: Avg Train Loss: 2.3017079830169678, Validation Loss: 2.3021414279937744\n",
            "Iteration 155: Avg Train Loss: 2.3016741275787354, Validation Loss: 2.302091121673584\n",
            "Iteration 156: Avg Train Loss: 2.301706075668335, Validation Loss: 2.3020424842834473\n",
            "Iteration 157: Avg Train Loss: 2.301628828048706, Validation Loss: 2.3020079135894775\n",
            "Iteration 158: Avg Train Loss: 2.301510810852051, Validation Loss: 2.3019485473632812\n",
            "Iteration 159: Avg Train Loss: 2.301483392715454, Validation Loss: 2.3018903732299805\n",
            "Iteration 160: Avg Train Loss: 2.3014495372772217, Validation Loss: 2.301818370819092\n",
            "Iteration 161: Avg Train Loss: 2.301365375518799, Validation Loss: 2.3017306327819824\n",
            "Iteration 162: Avg Train Loss: 2.3013179302215576, Validation Loss: 2.3016304969787598\n",
            "Iteration 163: Avg Train Loss: 2.301178455352783, Validation Loss: 2.3015310764312744\n",
            "Iteration 164: Avg Train Loss: 2.3010904788970947, Validation Loss: 2.301370859146118\n",
            "Iteration 165: Avg Train Loss: 2.300980567932129, Validation Loss: 2.3012216091156006\n",
            "Iteration 166: Avg Train Loss: 2.300865650177002, Validation Loss: 2.301053524017334\n",
            "Iteration 167: Avg Train Loss: 2.3006744384765625, Validation Loss: 2.3008599281311035\n",
            "Iteration 168: Avg Train Loss: 2.300488233566284, Validation Loss: 2.300640821456909\n",
            "Iteration 169: Avg Train Loss: 2.3003203868865967, Validation Loss: 2.300363063812256\n",
            "Iteration 170: Avg Train Loss: 2.2999987602233887, Validation Loss: 2.300045967102051\n",
            "Iteration 171: Avg Train Loss: 2.2997050285339355, Validation Loss: 2.299626588821411\n",
            "Iteration 172: Avg Train Loss: 2.29929780960083, Validation Loss: 2.2991061210632324\n",
            "Iteration 173: Avg Train Loss: 2.298757553100586, Validation Loss: 2.2984542846679688\n",
            "Iteration 174: Avg Train Loss: 2.298131227493286, Validation Loss: 2.297635078430176\n",
            "Iteration 175: Avg Train Loss: 2.2972652912139893, Validation Loss: 2.296522617340088\n",
            "Iteration 176: Avg Train Loss: 2.2961742877960205, Validation Loss: 2.2950186729431152\n",
            "Iteration 177: Avg Train Loss: 2.2945454120635986, Validation Loss: 2.2929069995880127\n",
            "Iteration 178: Avg Train Loss: 2.292360544204712, Validation Loss: 2.2897980213165283\n",
            "Iteration 179: Avg Train Loss: 2.288994073867798, Validation Loss: 2.2849581241607666\n",
            "Iteration 180: Avg Train Loss: 2.283738374710083, Validation Loss: 2.277196168899536\n",
            "Iteration 181: Avg Train Loss: 2.2746713161468506, Validation Loss: 2.2632319927215576\n",
            "Iteration 182: Avg Train Loss: 2.258326530456543, Validation Loss: 2.237018585205078\n",
            "Iteration 183: Avg Train Loss: 2.2273013591766357, Validation Loss: 2.189434766769409\n",
            "Iteration 184: Avg Train Loss: 2.178215742111206, Validation Loss: 2.1275320053100586\n",
            "Iteration 185: Avg Train Loss: 2.1216373443603516, Validation Loss: 2.0669894218444824\n",
            "Iteration 186: Avg Train Loss: 2.0686919689178467, Validation Loss: 2.014158248901367\n",
            "Iteration 187: Avg Train Loss: 2.0126497745513916, Validation Loss: 1.9534690380096436\n",
            "Iteration 188: Avg Train Loss: 1.947563886642456, Validation Loss: 1.8818795680999756\n",
            "Iteration 189: Avg Train Loss: 1.8679999113082886, Validation Loss: 1.7995357513427734\n",
            "Iteration 190: Avg Train Loss: 1.7821667194366455, Validation Loss: 1.726961612701416\n",
            "Iteration 191: Avg Train Loss: 1.7139440774917603, Validation Loss: 1.6658031940460205\n",
            "Iteration 192: Avg Train Loss: 1.6720095872879028, Validation Loss: 1.6394017934799194\n",
            "Iteration 193: Avg Train Loss: 1.6416131258010864, Validation Loss: 1.6171883344650269\n",
            "Iteration 194: Avg Train Loss: 1.6244884729385376, Validation Loss: 1.5963107347488403\n",
            "Iteration 195: Avg Train Loss: 1.6066498756408691, Validation Loss: 1.599517583847046\n",
            "Iteration 196: Avg Train Loss: 1.5909744501113892, Validation Loss: 1.5786350965499878\n",
            "Iteration 197: Avg Train Loss: 1.5763639211654663, Validation Loss: 1.5756312608718872\n",
            "Iteration 198: Avg Train Loss: 1.5672720670700073, Validation Loss: 1.5893975496292114\n",
            "Iteration 199: Avg Train Loss: 1.5561130046844482, Validation Loss: 1.5477386713027954\n",
            "Iteration 200: Avg Train Loss: 1.5411370992660522, Validation Loss: 1.5447744131088257\n",
            "Iteration 201: Avg Train Loss: 1.5275306701660156, Validation Loss: 1.5755215883255005\n",
            "Iteration 202: Avg Train Loss: 1.5180820226669312, Validation Loss: 1.5356580018997192\n",
            "Iteration 203: Avg Train Loss: 1.501072883605957, Validation Loss: 1.5057631731033325\n",
            "Iteration 204: Avg Train Loss: 1.4820995330810547, Validation Loss: 1.495192050933838\n",
            "Iteration 205: Avg Train Loss: 1.4696400165557861, Validation Loss: 1.474551796913147\n",
            "Iteration 206: Avg Train Loss: 1.4534298181533813, Validation Loss: 1.469447374343872\n",
            "Iteration 207: Avg Train Loss: 1.4338788986206055, Validation Loss: 1.5274678468704224\n",
            "Iteration 208: Avg Train Loss: 1.4133857488632202, Validation Loss: 1.4533392190933228\n",
            "Iteration 209: Avg Train Loss: 1.3969045877456665, Validation Loss: 1.4472073316574097\n",
            "Iteration 210: Avg Train Loss: 1.3738837242126465, Validation Loss: 1.3901175260543823\n",
            "Iteration 211: Avg Train Loss: 1.3531086444854736, Validation Loss: 1.3772553205490112\n",
            "Iteration 212: Avg Train Loss: 1.3319474458694458, Validation Loss: 1.3655858039855957\n",
            "Iteration 213: Avg Train Loss: 1.3116660118103027, Validation Loss: 1.3486162424087524\n",
            "Iteration 214: Avg Train Loss: 1.2928506135940552, Validation Loss: 1.331972599029541\n",
            "Iteration 215: Avg Train Loss: 1.2653863430023193, Validation Loss: 1.3203853368759155\n",
            "Iteration 216: Avg Train Loss: 1.2499442100524902, Validation Loss: 1.3441731929779053\n",
            "Iteration 217: Avg Train Loss: 1.2262861728668213, Validation Loss: 1.2790344953536987\n",
            "Iteration 218: Avg Train Loss: 1.203583002090454, Validation Loss: 1.252245306968689\n",
            "Iteration 219: Avg Train Loss: 1.179626703262329, Validation Loss: 1.273492693901062\n",
            "Iteration 220: Avg Train Loss: 1.1593860387802124, Validation Loss: 1.2198255062103271\n",
            "Iteration 221: Avg Train Loss: 1.1371363401412964, Validation Loss: 1.2000473737716675\n",
            "Iteration 222: Avg Train Loss: 1.1119728088378906, Validation Loss: 1.1981141567230225\n",
            "Iteration 223: Avg Train Loss: 1.0889824628829956, Validation Loss: 1.1830763816833496\n",
            "Iteration 224: Avg Train Loss: 1.0671391487121582, Validation Loss: 1.1920287609100342\n",
            "Iteration 225: Avg Train Loss: 1.0475183725357056, Validation Loss: 1.1316195726394653\n",
            "Iteration 226: Avg Train Loss: 1.0165421962738037, Validation Loss: 1.0980478525161743\n",
            "Iteration 227: Avg Train Loss: 0.9861280918121338, Validation Loss: 1.1526600122451782\n",
            "Iteration 228: Avg Train Loss: 0.9678456783294678, Validation Loss: 1.0977239608764648\n",
            "Iteration 229: Avg Train Loss: 0.9394419193267822, Validation Loss: 1.0672578811645508\n",
            "Iteration 230: Avg Train Loss: 0.9116736054420471, Validation Loss: 1.06171715259552\n",
            "Iteration 231: Avg Train Loss: 0.891390323638916, Validation Loss: 1.0443543195724487\n",
            "Iteration 232: Avg Train Loss: 0.8565232157707214, Validation Loss: 1.0080783367156982\n",
            "Iteration 233: Avg Train Loss: 0.8214415311813354, Validation Loss: 1.1147258281707764\n",
            "Iteration 234: Avg Train Loss: 0.7949439287185669, Validation Loss: 0.9271824955940247\n",
            "Iteration 235: Avg Train Loss: 0.7521476149559021, Validation Loss: 0.9333091974258423\n",
            "Iteration 236: Avg Train Loss: 0.713636577129364, Validation Loss: 0.9431816339492798\n",
            "Iteration 237: Avg Train Loss: 0.666527509689331, Validation Loss: 0.8795918822288513\n",
            "Iteration 238: Avg Train Loss: 0.6380361318588257, Validation Loss: 0.881049633026123\n",
            "Iteration 239: Avg Train Loss: 0.5906537771224976, Validation Loss: 0.874240517616272\n",
            "Iteration 240: Avg Train Loss: 0.5530946254730225, Validation Loss: 0.7745652198791504\n",
            "Iteration 241: Avg Train Loss: 0.5197703242301941, Validation Loss: 0.7805158495903015\n",
            "Iteration 242: Avg Train Loss: 0.4943382740020752, Validation Loss: 0.7335642576217651\n",
            "Iteration 243: Avg Train Loss: 0.45424211025238037, Validation Loss: 0.7142447233200073\n",
            "Iteration 244: Avg Train Loss: 0.45355620980262756, Validation Loss: 0.7129361629486084\n",
            "Iteration 245: Avg Train Loss: 0.4065551161766052, Validation Loss: 0.7417303323745728\n",
            "Iteration 246: Avg Train Loss: 0.3916207253932953, Validation Loss: 0.7083858847618103\n",
            "Iteration 247: Avg Train Loss: 0.3592653274536133, Validation Loss: 0.7294266819953918\n",
            "Iteration 248: Avg Train Loss: 0.3406718075275421, Validation Loss: 0.6895346641540527\n",
            "Iteration 249: Avg Train Loss: 0.3130711615085602, Validation Loss: 0.704443633556366\n",
            "Iteration 250: Avg Train Loss: 0.30338549613952637, Validation Loss: 0.6744402050971985\n",
            "Iteration 251: Avg Train Loss: 0.29445454478263855, Validation Loss: 0.7120303511619568\n",
            "Iteration 252: Avg Train Loss: 0.26187217235565186, Validation Loss: 0.6871346235275269\n",
            "Iteration 253: Avg Train Loss: 0.2432311475276947, Validation Loss: 0.6782205104827881\n",
            "Iteration 254: Avg Train Loss: 0.23162929713726044, Validation Loss: 0.765967071056366\n",
            "Iteration 255: Avg Train Loss: 0.2255014330148697, Validation Loss: 0.6720163822174072\n",
            "Iteration 256: Avg Train Loss: 0.20373331010341644, Validation Loss: 0.6643014550209045\n",
            "Iteration 257: Avg Train Loss: 0.19608640670776367, Validation Loss: 0.6783081293106079\n",
            "Iteration 258: Avg Train Loss: 0.18585970997810364, Validation Loss: 0.6550809144973755\n",
            "Iteration 259: Avg Train Loss: 0.16434521973133087, Validation Loss: 0.6703654527664185\n",
            "Iteration 260: Avg Train Loss: 0.18288582563400269, Validation Loss: 0.7026163339614868\n",
            "Iteration 261: Avg Train Loss: 0.14914757013320923, Validation Loss: 0.6943431496620178\n",
            "Iteration 262: Avg Train Loss: 0.14672774076461792, Validation Loss: 0.6821898221969604\n",
            "Iteration 263: Avg Train Loss: 0.12887777388095856, Validation Loss: 0.7123603224754333\n",
            "Iteration 264: Avg Train Loss: 0.11993711441755295, Validation Loss: 0.6964649558067322\n",
            "Iteration 265: Avg Train Loss: 0.10812702029943466, Validation Loss: 0.6923114657402039\n",
            "Iteration 266: Avg Train Loss: 0.10269150137901306, Validation Loss: 0.8298724293708801\n",
            "Iteration 267: Avg Train Loss: 0.09404376894235611, Validation Loss: 0.7094753384590149\n",
            "Iteration 268: Avg Train Loss: 0.09150264412164688, Validation Loss: 0.7139514088630676\n",
            "Iteration 269: Avg Train Loss: 0.08532758057117462, Validation Loss: 0.8150520920753479\n",
            "Iteration 270: Avg Train Loss: 0.07881799340248108, Validation Loss: 0.7363073229789734\n",
            "Iteration 271: Avg Train Loss: 0.0720401480793953, Validation Loss: 0.728993833065033\n",
            "Iteration 272: Avg Train Loss: 0.06723547726869583, Validation Loss: 0.7487439513206482\n",
            "Iteration 273: Avg Train Loss: 0.06420823186635971, Validation Loss: 0.7572411894798279\n",
            "Iteration 274: Avg Train Loss: 0.06333384662866592, Validation Loss: 0.7595017552375793\n",
            "Iteration 275: Avg Train Loss: 0.0553988516330719, Validation Loss: 0.764797031879425\n",
            "Iteration 276: Avg Train Loss: 0.0505107082426548, Validation Loss: 0.7605355381965637\n",
            "Iteration 277: Avg Train Loss: 0.04818545654416084, Validation Loss: 0.7703969478607178\n",
            "Iteration 278: Avg Train Loss: 0.04502183571457863, Validation Loss: 0.7674650549888611\n",
            "Iteration 279: Avg Train Loss: 0.04136146977543831, Validation Loss: 0.7876273393630981\n",
            "Iteration 280: Avg Train Loss: 0.03977728262543678, Validation Loss: 0.7815746665000916\n",
            "Iteration 281: Avg Train Loss: 0.03705798462033272, Validation Loss: 0.7865817546844482\n",
            "Iteration 282: Avg Train Loss: 0.03510379418730736, Validation Loss: 0.7926769852638245\n",
            "Iteration 283: Avg Train Loss: 0.032281797379255295, Validation Loss: 0.813389241695404\n",
            "Iteration 284: Avg Train Loss: 0.030426371842622757, Validation Loss: 0.8000681400299072\n",
            "Iteration 285: Avg Train Loss: 0.02831021137535572, Validation Loss: 0.8180655837059021\n",
            "Iteration 286: Avg Train Loss: 0.027044912800192833, Validation Loss: 0.8198131918907166\n",
            "Iteration 287: Avg Train Loss: 0.02554531954228878, Validation Loss: 0.8189123272895813\n",
            "Iteration 288: Avg Train Loss: 0.024086114019155502, Validation Loss: 0.8156555891036987\n",
            "Iteration 289: Avg Train Loss: 0.023215070366859436, Validation Loss: 0.8225094676017761\n",
            "Iteration 290: Avg Train Loss: 0.021550951525568962, Validation Loss: 0.8415334224700928\n",
            "Iteration 291: Avg Train Loss: 0.020571226254105568, Validation Loss: 0.8480498194694519\n",
            "Iteration 292: Avg Train Loss: 0.019542284309864044, Validation Loss: 0.8448312282562256\n",
            "Iteration 293: Avg Train Loss: 0.018663981929421425, Validation Loss: 0.8501362800598145\n",
            "Iteration 294: Avg Train Loss: 0.017813898622989655, Validation Loss: 0.8507004380226135\n",
            "Iteration 295: Avg Train Loss: 0.016970543190836906, Validation Loss: 0.8534405827522278\n",
            "Iteration 296: Avg Train Loss: 0.016419589519500732, Validation Loss: 0.8783940076828003\n",
            "Iteration 297: Avg Train Loss: 0.015486265532672405, Validation Loss: 0.8618977665901184\n",
            "Iteration 298: Avg Train Loss: 0.014967961236834526, Validation Loss: 0.8686633706092834\n",
            "Iteration 299: Avg Train Loss: 0.014601191505789757, Validation Loss: 0.8686883449554443\n",
            "Iteration 300: Avg Train Loss: 0.013843847438693047, Validation Loss: 0.8768067955970764\n",
            "Iteration 301: Avg Train Loss: 0.013258837163448334, Validation Loss: 0.8791467547416687\n",
            "Iteration 302: Avg Train Loss: 0.012854541651904583, Validation Loss: 0.8828529119491577\n",
            "Iteration 303: Avg Train Loss: 0.01226827036589384, Validation Loss: 0.879830002784729\n",
            "Iteration 304: Avg Train Loss: 0.011849268339574337, Validation Loss: 0.8874425888061523\n",
            "Iteration 305: Avg Train Loss: 0.011422176845371723, Validation Loss: 0.8883145451545715\n",
            "Iteration 306: Avg Train Loss: 0.011099784635007381, Validation Loss: 0.8962543606758118\n",
            "Iteration 307: Avg Train Loss: 0.010736347176134586, Validation Loss: 0.8946381211280823\n",
            "Iteration 308: Avg Train Loss: 0.010382688604295254, Validation Loss: 0.8996220827102661\n",
            "Iteration 309: Avg Train Loss: 0.010104888118803501, Validation Loss: 0.9066384434700012\n",
            "Iteration 310: Avg Train Loss: 0.009741103276610374, Validation Loss: 0.9076316952705383\n",
            "Iteration 311: Avg Train Loss: 0.009495416656136513, Validation Loss: 0.9083331823348999\n",
            "Iteration 312: Avg Train Loss: 0.009149778634309769, Validation Loss: 0.9094167351722717\n",
            "Iteration 313: Avg Train Loss: 0.008865172043442726, Validation Loss: 0.9111656546592712\n",
            "Iteration 314: Avg Train Loss: 0.008727841079235077, Validation Loss: 0.9166547656059265\n",
            "Iteration 315: Avg Train Loss: 0.00840730406343937, Validation Loss: 0.9196525812149048\n",
            "Iteration 316: Avg Train Loss: 0.00819351989775896, Validation Loss: 0.9241600036621094\n",
            "Iteration 317: Avg Train Loss: 0.007978086359798908, Validation Loss: 0.9273123741149902\n",
            "Iteration 318: Avg Train Loss: 0.007792294025421143, Validation Loss: 0.9306384921073914\n",
            "Iteration 319: Avg Train Loss: 0.00756399892270565, Validation Loss: 0.9292417168617249\n",
            "Iteration 320: Avg Train Loss: 0.007329635322093964, Validation Loss: 0.9342377185821533\n",
            "Iteration 321: Avg Train Loss: 0.00719403987750411, Validation Loss: 0.9375994205474854\n",
            "Iteration 322: Avg Train Loss: 0.006989565212279558, Validation Loss: 0.9430413842201233\n",
            "Iteration 323: Avg Train Loss: 0.006819052156060934, Validation Loss: 0.9451848268508911\n",
            "Iteration 324: Avg Train Loss: 0.006640540435910225, Validation Loss: 0.9407525658607483\n",
            "Iteration 325: Avg Train Loss: 0.006516857072710991, Validation Loss: 0.948907196521759\n",
            "Iteration 326: Avg Train Loss: 0.0063515473157167435, Validation Loss: 0.9449408650398254\n",
            "Iteration 327: Avg Train Loss: 0.0062318481504917145, Validation Loss: 0.9499961137771606\n",
            "Iteration 328: Avg Train Loss: 0.006043537054210901, Validation Loss: 0.9521264433860779\n",
            "Iteration 329: Avg Train Loss: 0.005936719477176666, Validation Loss: 0.9578375816345215\n",
            "Iteration 330: Avg Train Loss: 0.005818069446831942, Validation Loss: 0.9608447551727295\n",
            "Iteration 331: Avg Train Loss: 0.005671168677508831, Validation Loss: 0.9593509435653687\n",
            "Iteration 332: Avg Train Loss: 0.00557516235858202, Validation Loss: 0.9596474170684814\n",
            "Iteration 333: Avg Train Loss: 0.0054241567850112915, Validation Loss: 0.9672042727470398\n",
            "Iteration 334: Avg Train Loss: 0.005343524273484945, Validation Loss: 0.9657289385795593\n",
            "Iteration 335: Avg Train Loss: 0.005217944271862507, Validation Loss: 0.9668124318122864\n",
            "Iteration 336: Avg Train Loss: 0.005124297458678484, Validation Loss: 0.9739950895309448\n",
            "Iteration 337: Avg Train Loss: 0.005017404444515705, Validation Loss: 0.972720742225647\n",
            "Iteration 338: Avg Train Loss: 0.00491627911105752, Validation Loss: 0.9751275777816772\n",
            "Iteration 339: Avg Train Loss: 0.004838400520384312, Validation Loss: 0.9781163334846497\n",
            "Iteration 340: Avg Train Loss: 0.004739600233733654, Validation Loss: 0.9780409932136536\n",
            "Iteration 341: Avg Train Loss: 0.004642255138605833, Validation Loss: 0.9785711765289307\n",
            "Iteration 342: Avg Train Loss: 0.00458445493131876, Validation Loss: 0.981303870677948\n",
            "Iteration 343: Avg Train Loss: 0.004484943579882383, Validation Loss: 0.9842029809951782\n",
            "Iteration 344: Avg Train Loss: 0.004403517581522465, Validation Loss: 0.9879075884819031\n",
            "Iteration 345: Avg Train Loss: 0.004339301958680153, Validation Loss: 0.986704409122467\n",
            "Iteration 346: Avg Train Loss: 0.004239307716488838, Validation Loss: 0.9884264469146729\n",
            "Iteration 347: Avg Train Loss: 0.004175378009676933, Validation Loss: 0.9910773038864136\n",
            "Iteration 348: Avg Train Loss: 0.004105237778276205, Validation Loss: 0.9874358773231506\n",
            "Iteration 349: Avg Train Loss: 0.00405975291505456, Validation Loss: 0.997054934501648\n",
            "Iteration 350: Avg Train Loss: 0.003969471901655197, Validation Loss: 0.9961671829223633\n",
            "Iteration 351: Avg Train Loss: 0.003911005798727274, Validation Loss: 0.9943079352378845\n",
            "Iteration 352: Avg Train Loss: 0.0038564763963222504, Validation Loss: 1.0040946006774902\n",
            "Iteration 353: Avg Train Loss: 0.0037805901374667883, Validation Loss: 1.0008280277252197\n",
            "Iteration 354: Avg Train Loss: 0.0037170909345149994, Validation Loss: 1.003463864326477\n",
            "Iteration 355: Avg Train Loss: 0.00366237573325634, Validation Loss: 1.0045491456985474\n",
            "Iteration 356: Avg Train Loss: 0.003627987578511238, Validation Loss: 1.0027313232421875\n",
            "Iteration 357: Avg Train Loss: 0.003548562992364168, Validation Loss: 1.0071321725845337\n",
            "Iteration 358: Avg Train Loss: 0.0035001300275325775, Validation Loss: 1.0067830085754395\n",
            "Iteration 359: Avg Train Loss: 0.003459224244579673, Validation Loss: 1.0103179216384888\n",
            "Iteration 360: Avg Train Loss: 0.0034001448657363653, Validation Loss: 1.0108697414398193\n",
            "Iteration 361: Avg Train Loss: 0.0033622828777879477, Validation Loss: 1.0124329328536987\n",
            "Iteration 362: Avg Train Loss: 0.003308637998998165, Validation Loss: 1.0162280797958374\n",
            "Iteration 363: Avg Train Loss: 0.0032602495048195124, Validation Loss: 1.0141876935958862\n",
            "Iteration 364: Avg Train Loss: 0.0032120677642524242, Validation Loss: 1.0195424556732178\n",
            "Iteration 365: Avg Train Loss: 0.0031720183324068785, Validation Loss: 1.0197532176971436\n",
            "Iteration 366: Avg Train Loss: 0.0031217848882079124, Validation Loss: 1.0218827724456787\n",
            "Iteration 367: Avg Train Loss: 0.0030744795221835375, Validation Loss: 1.0218158960342407\n",
            "Iteration 368: Avg Train Loss: 0.003033417509868741, Validation Loss: 1.0214133262634277\n",
            "Iteration 369: Avg Train Loss: 0.002990291453897953, Validation Loss: 1.0252315998077393\n",
            "Iteration 370: Avg Train Loss: 0.0029579815454781055, Validation Loss: 1.026260256767273\n",
            "Iteration 371: Avg Train Loss: 0.002915280172601342, Validation Loss: 1.0239859819412231\n",
            "Iteration 372: Avg Train Loss: 0.0028809572104364634, Validation Loss: 1.028398036956787\n",
            "Iteration 373: Avg Train Loss: 0.0028482016641646624, Validation Loss: 1.0279338359832764\n",
            "Iteration 374: Avg Train Loss: 0.0028079096227884293, Validation Loss: 1.032193899154663\n",
            "Iteration 375: Avg Train Loss: 0.0027715014293789864, Validation Loss: 1.030635118484497\n",
            "Iteration 376: Avg Train Loss: 0.0027426849119365215, Validation Loss: 1.0330219268798828\n",
            "Iteration 377: Avg Train Loss: 0.002698622876778245, Validation Loss: 1.0348221063613892\n",
            "Iteration 378: Avg Train Loss: 0.0026760860346257687, Validation Loss: 1.0372930765151978\n",
            "Iteration 379: Avg Train Loss: 0.002644360763952136, Validation Loss: 1.0384410619735718\n",
            "Iteration 380: Avg Train Loss: 0.002607284812256694, Validation Loss: 1.035790205001831\n",
            "Iteration 381: Avg Train Loss: 0.0025715057272464037, Validation Loss: 1.040068507194519\n",
            "Iteration 382: Avg Train Loss: 0.002543687354773283, Validation Loss: 1.0413182973861694\n",
            "Iteration 383: Avg Train Loss: 0.0025214110501110554, Validation Loss: 1.0411794185638428\n",
            "Iteration 384: Avg Train Loss: 0.0024791534524410963, Validation Loss: 1.0422426462173462\n",
            "Iteration 385: Avg Train Loss: 0.002453544409945607, Validation Loss: 1.044750452041626\n",
            "Iteration 386: Avg Train Loss: 0.0024247707333415747, Validation Loss: 1.0459257364273071\n",
            "Iteration 387: Avg Train Loss: 0.002413735957816243, Validation Loss: 1.047454595565796\n",
            "Iteration 388: Avg Train Loss: 0.002370053669437766, Validation Loss: 1.0469621419906616\n",
            "Iteration 389: Avg Train Loss: 0.0023495496716350317, Validation Loss: 1.048997163772583\n",
            "Iteration 390: Avg Train Loss: 0.0023248200304806232, Validation Loss: 1.0503835678100586\n",
            "Iteration 391: Avg Train Loss: 0.002287565963342786, Validation Loss: 1.0506762266159058\n",
            "Iteration 392: Avg Train Loss: 0.0022651355247944593, Validation Loss: 1.049875020980835\n",
            "Iteration 393: Avg Train Loss: 0.002246163785457611, Validation Loss: 1.0526186227798462\n",
            "Iteration 394: Avg Train Loss: 0.0022195465862751007, Validation Loss: 1.0536110401153564\n",
            "Iteration 395: Avg Train Loss: 0.0021986928768455982, Validation Loss: 1.056071162223816\n",
            "Iteration 396: Avg Train Loss: 0.002174893394112587, Validation Loss: 1.0556238889694214\n",
            "Iteration 397: Avg Train Loss: 0.0021619040053337812, Validation Loss: 1.0588163137435913\n",
            "Iteration 398: Avg Train Loss: 0.0021320877131074667, Validation Loss: 1.0594310760498047\n",
            "Iteration 399: Avg Train Loss: 0.0021073371171951294, Validation Loss: 1.060378909111023\n",
            "Iteration 400: Avg Train Loss: 0.002084065228700638, Validation Loss: 1.0609111785888672\n",
            "Iteration 401: Avg Train Loss: 0.0020702569745481014, Validation Loss: 1.0619189739227295\n",
            "Iteration 402: Avg Train Loss: 0.0020543639548122883, Validation Loss: 1.062341332435608\n",
            "Iteration 403: Avg Train Loss: 0.0020219432190060616, Validation Loss: 1.063306450843811\n",
            "Iteration 404: Avg Train Loss: 0.002003055065870285, Validation Loss: 1.0646321773529053\n",
            "Iteration 405: Avg Train Loss: 0.0019868386443704367, Validation Loss: 1.065946340560913\n",
            "Iteration 406: Avg Train Loss: 0.0019640086684376, Validation Loss: 1.065425992012024\n",
            "Iteration 407: Avg Train Loss: 0.001957583473995328, Validation Loss: 1.0669851303100586\n",
            "Iteration 408: Avg Train Loss: 0.0019286773167550564, Validation Loss: 1.0696053504943848\n",
            "Iteration 409: Avg Train Loss: 0.001910436782054603, Validation Loss: 1.070689082145691\n",
            "Iteration 410: Avg Train Loss: 0.0018915642285719514, Validation Loss: 1.070082664489746\n",
            "Iteration 411: Avg Train Loss: 0.0018785498104989529, Validation Loss: 1.0726219415664673\n",
            "Iteration 412: Avg Train Loss: 0.0018585051875561476, Validation Loss: 1.0718793869018555\n",
            "Iteration 413: Avg Train Loss: 0.0018432318465784192, Validation Loss: 1.074344515800476\n",
            "Iteration 414: Avg Train Loss: 0.00181791209615767, Validation Loss: 1.0752865076065063\n",
            "Iteration 415: Avg Train Loss: 0.0018024693708866835, Validation Loss: 1.0743879079818726\n",
            "Iteration 416: Avg Train Loss: 0.0017922442639246583, Validation Loss: 1.0757893323898315\n",
            "Iteration 417: Avg Train Loss: 0.001772476825863123, Validation Loss: 1.0804024934768677\n",
            "Iteration 418: Avg Train Loss: 0.0017549714539200068, Validation Loss: 1.0773805379867554\n",
            "Iteration 419: Avg Train Loss: 0.0017469821032136679, Validation Loss: 1.0798841714859009\n",
            "Iteration 420: Avg Train Loss: 0.0017307893140241504, Validation Loss: 1.079403281211853\n",
            "Iteration 421: Avg Train Loss: 0.001713847042992711, Validation Loss: 1.0809904336929321\n",
            "Iteration 422: Avg Train Loss: 0.001693084603175521, Validation Loss: 1.0808579921722412\n",
            "Iteration 423: Avg Train Loss: 0.0016826583305373788, Validation Loss: 1.0827113389968872\n",
            "Iteration 424: Avg Train Loss: 0.0016676712548360229, Validation Loss: 1.0838216543197632\n",
            "Iteration 425: Avg Train Loss: 0.001660963287577033, Validation Loss: 1.0840028524398804\n",
            "Iteration 426: Avg Train Loss: 0.001637757639400661, Validation Loss: 1.0857007503509521\n",
            "Iteration 427: Avg Train Loss: 0.001624392345547676, Validation Loss: 1.0858806371688843\n",
            "Iteration 428: Avg Train Loss: 0.0016163528198376298, Validation Loss: 1.0878599882125854\n",
            "Iteration 429: Avg Train Loss: 0.001605733996257186, Validation Loss: 1.0886305570602417\n",
            "Iteration 430: Avg Train Loss: 0.001587396371178329, Validation Loss: 1.088577389717102\n",
            "Iteration 431: Avg Train Loss: 0.0015732281608507037, Validation Loss: 1.090860366821289\n",
            "Iteration 432: Avg Train Loss: 0.0015587604139000177, Validation Loss: 1.091317057609558\n",
            "Iteration 433: Avg Train Loss: 0.001552604720927775, Validation Loss: 1.0913008451461792\n",
            "Iteration 434: Avg Train Loss: 0.0015330560272559524, Validation Loss: 1.0913102626800537\n",
            "Iteration 435: Avg Train Loss: 0.001523538026958704, Validation Loss: 1.092570185661316\n",
            "Iteration 436: Avg Train Loss: 0.0015077757416293025, Validation Loss: 1.0919798612594604\n",
            "Iteration 437: Avg Train Loss: 0.0015045743202790618, Validation Loss: 1.0957401990890503\n",
            "Iteration 438: Avg Train Loss: 0.0014868524158373475, Validation Loss: 1.0954228639602661\n",
            "Iteration 439: Avg Train Loss: 0.0014752645511180162, Validation Loss: 1.095245599746704\n",
            "Iteration 440: Avg Train Loss: 0.0014671609969809651, Validation Loss: 1.0965832471847534\n",
            "Iteration 441: Avg Train Loss: 0.0014527004677802324, Validation Loss: 1.0989011526107788\n",
            "Iteration 442: Avg Train Loss: 0.0014414368197321892, Validation Loss: 1.0986322164535522\n",
            "Iteration 443: Avg Train Loss: 0.0014291520928964019, Validation Loss: 1.0982600450515747\n",
            "Iteration 444: Avg Train Loss: 0.0014197338605299592, Validation Loss: 1.0995922088623047\n",
            "Iteration 445: Avg Train Loss: 0.001411012839525938, Validation Loss: 1.100865364074707\n",
            "Iteration 446: Avg Train Loss: 0.0013987700222060084, Validation Loss: 1.101254940032959\n",
            "Iteration 447: Avg Train Loss: 0.0013890950940549374, Validation Loss: 1.1020102500915527\n",
            "Iteration 448: Avg Train Loss: 0.0013796406565234065, Validation Loss: 1.101485013961792\n",
            "Iteration 449: Avg Train Loss: 0.0013698902912437916, Validation Loss: 1.103811264038086\n",
            "Iteration 450: Avg Train Loss: 0.0013602168764919043, Validation Loss: 1.1046825647354126\n",
            "Iteration 451: Avg Train Loss: 0.001352562801912427, Validation Loss: 1.104230523109436\n",
            "Iteration 452: Avg Train Loss: 0.0013390679378062487, Validation Loss: 1.105607509613037\n",
            "Iteration 453: Avg Train Loss: 0.0013316043186932802, Validation Loss: 1.1063497066497803\n",
            "Iteration 454: Avg Train Loss: 0.0013191007310524583, Validation Loss: 1.1058158874511719\n",
            "Iteration 455: Avg Train Loss: 0.0013126275734975934, Validation Loss: 1.1069130897521973\n",
            "Iteration 456: Avg Train Loss: 0.001302990480326116, Validation Loss: 1.1075859069824219\n",
            "Iteration 457: Avg Train Loss: 0.0012956379214301705, Validation Loss: 1.1086912155151367\n",
            "Iteration 458: Avg Train Loss: 0.0012842919677495956, Validation Loss: 1.1096079349517822\n",
            "Iteration 459: Avg Train Loss: 0.0012724329717457294, Validation Loss: 1.112123727798462\n",
            "Iteration 460: Avg Train Loss: 0.001265966915525496, Validation Loss: 1.110304355621338\n",
            "Iteration 461: Avg Train Loss: 0.0012574737193062901, Validation Loss: 1.1111537218093872\n",
            "Iteration 462: Avg Train Loss: 0.0012510718079283834, Validation Loss: 1.1121681928634644\n",
            "Iteration 463: Avg Train Loss: 0.0012441289145499468, Validation Loss: 1.1135751008987427\n",
            "Iteration 464: Avg Train Loss: 0.0012309367302805185, Validation Loss: 1.113419771194458\n",
            "Iteration 465: Avg Train Loss: 0.0012227591359987855, Validation Loss: 1.1142712831497192\n",
            "Iteration 466: Avg Train Loss: 0.0012138804886490107, Validation Loss: 1.1144598722457886\n",
            "Iteration 467: Avg Train Loss: 0.0012074258411303163, Validation Loss: 1.1166256666183472\n",
            "Iteration 468: Avg Train Loss: 0.0012008391786366701, Validation Loss: 1.1164846420288086\n",
            "Iteration 469: Avg Train Loss: 0.0011919587850570679, Validation Loss: 1.116924524307251\n",
            "Iteration 470: Avg Train Loss: 0.00118157546967268, Validation Loss: 1.1162863969802856\n",
            "Iteration 471: Avg Train Loss: 0.001178682898171246, Validation Loss: 1.117944598197937\n",
            "Iteration 472: Avg Train Loss: 0.001169749884866178, Validation Loss: 1.1196446418762207\n",
            "Iteration 473: Avg Train Loss: 0.0011596960248425603, Validation Loss: 1.1194947957992554\n",
            "Iteration 474: Avg Train Loss: 0.001154345110990107, Validation Loss: 1.121521234512329\n",
            "Iteration 475: Avg Train Loss: 0.0011444217525422573, Validation Loss: 1.1216000318527222\n",
            "Iteration 476: Avg Train Loss: 0.0011390515137463808, Validation Loss: 1.121689796447754\n",
            "Iteration 477: Avg Train Loss: 0.0011313819559291005, Validation Loss: 1.1229710578918457\n",
            "Iteration 478: Avg Train Loss: 0.001122968504205346, Validation Loss: 1.122312307357788\n",
            "Iteration 479: Avg Train Loss: 0.0011185876792296767, Validation Loss: 1.123464584350586\n",
            "Iteration 480: Avg Train Loss: 0.0011100820265710354, Validation Loss: 1.124130368232727\n",
            "Iteration 481: Avg Train Loss: 0.00110072479583323, Validation Loss: 1.1240193843841553\n",
            "Iteration 482: Avg Train Loss: 0.001098877051845193, Validation Loss: 1.1256190538406372\n",
            "Iteration 483: Avg Train Loss: 0.0010884952498599887, Validation Loss: 1.1257904767990112\n",
            "Iteration 484: Avg Train Loss: 0.0010826578363776207, Validation Loss: 1.128344178199768\n",
            "Iteration 485: Avg Train Loss: 0.0010751436930149794, Validation Loss: 1.1263102293014526\n",
            "Iteration 486: Avg Train Loss: 0.0010714040836319327, Validation Loss: 1.1277003288269043\n",
            "Iteration 487: Avg Train Loss: 0.0010680712293833494, Validation Loss: 1.1286770105361938\n",
            "Iteration 488: Avg Train Loss: 0.0010598927037790418, Validation Loss: 1.129103422164917\n",
            "Iteration 489: Avg Train Loss: 0.0010490467539057136, Validation Loss: 1.128888726234436\n",
            "Iteration 490: Avg Train Loss: 0.0010438770987093449, Validation Loss: 1.1303250789642334\n",
            "Iteration 491: Avg Train Loss: 0.0010381618048995733, Validation Loss: 1.1323716640472412\n",
            "Iteration 492: Avg Train Loss: 0.0010365215130150318, Validation Loss: 1.1326485872268677\n",
            "Iteration 493: Avg Train Loss: 0.0010277838446199894, Validation Loss: 1.1317665576934814\n",
            "Iteration 494: Avg Train Loss: 0.0010202840203419328, Validation Loss: 1.133423924446106\n",
            "Iteration 495: Avg Train Loss: 0.0010141328675672412, Validation Loss: 1.1329752206802368\n",
            "Iteration 496: Avg Train Loss: 0.0010116156190633774, Validation Loss: 1.1342427730560303\n",
            "Iteration 497: Avg Train Loss: 0.0010044939117506146, Validation Loss: 1.1342159509658813\n",
            "Iteration 498: Avg Train Loss: 0.0009977056179195642, Validation Loss: 1.135872483253479\n",
            "Iteration 499: Avg Train Loss: 0.000994891976006329, Validation Loss: 1.136950135231018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=500\n",
        "plt.plot(range(0,iterations),train_history,'b')\n",
        "plt.plot(range(0,iterations),val_history,'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "FulFPHDw1DD0",
        "outputId": "dd785282-cdd5-4981-9f41-500676a06779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With\n",
        "\n",
        "**[512,256,128,10], 0.01, 500,32:**\n",
        "\n",
        "**After 300th epoch, training loss flattened after reaching to a value of 0% whereas val_loss increased after 300 th epoch but the loss value is than that of the initial loss. **\n",
        "\n",
        "The losses reachedto around 0% with alower learning rate(0.01) and in a fewer number of epochs (500) in batch gradient descent than with a netwrk without the minibatch gradient as it took more than 2000epochs at a learning rate of 0.01 to get toaower loss values.\n",
        "\n",
        "Other experiments\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "[512,256,128,10], 0.1, 250,32:\n",
        "\n",
        "Reached to 0% by 140th epoch, training loss started increasing after 140th epoch. val loss reached toa value of 0.57 and fluctuated around that range after 140th epoch.\n",
        "\n",
        "[512,256,128,10],lr= 0.1, 150,batch size=128, losses reached to a value of 0% and flattened by the end of last epochs with lot of fluctuations. Val_loss reacged 0.64 whereas train_lossreahed 0.002%\n",
        "\n",
        "batch_size = 32  # Example batch size\n",
        "learning_rate = 0.5\n",
        "iterations = 50  # Number of epochs\n",
        "nh = [512, 256, 128, 10], losses reached plateau toavalue of around 0% but with big spikes (fluctuations) before reaching there.\n",
        "\n",
        "\n",
        "batch_size = 64  # Example batch size\n",
        "learning_rate = 0.01\n",
        "iterations = 50  # Number of epochs\n",
        "nh = [512, 256, 128, 10], losses reached  a value of around 2.3%.\n",
        "\n",
        "batch_size = 128  # Example batch size\n",
        "learning_rate = 0.01\n",
        "iterations = 50  & 150# Number of epochs\n",
        "nh = [512, 256, 128, 10], losses reached  a value of around 2.3%.\n",
        "\n",
        "batch_size = 32  # Example batch size\n",
        "learning_rate = 0.01\n",
        "iterations = 50  # Number of epochs\n",
        "nh = [512, 256, 128, 10], losses reached  a value of around 2.3%.\n",
        "\n",
        "batch_size = 64  # Example batch size\n",
        "learning_rate = 0.1\n",
        "iterations = 50  # Number of epochs\n",
        "nh = [512, 256, 128, 10], losses converged till 35 the epoch and at 50th epoch recorded  a value of around 0.63% but with big spikes (fluctuations) .\n",
        "\n",
        "batch_size = 32  # Example batch size\n",
        "learning_rate = 0.1\n",
        "iterations = 50  # Number of epochs\n",
        "nh = [512, 256, 128, 10], training loss recorded  a value of around 0% but val_loss reached there with spikes (fluctuations) and flattened aftr 40th epoch.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-MUyeK_Rk197"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#2- Update parameters using Nesterov Momentum\n",
        "\n",
        ". The update rule for nesterov Momentum is as follows:\n",
        "𝑣 = 𝜇 ∗ 𝑣 − 𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑅𝑎𝑡𝑒 ∗ 𝑔\n",
        "𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟 = 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟 + 𝜇 ∗ 𝑣 − 𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑅𝑎𝑡𝑒 ∗ 𝑔\n",
        "Where 𝑔 is the gradient of loss with respect to the\n",
        "𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟 𝑦𝑜𝑢 𝑤𝑎𝑛𝑡 𝑡𝑜 𝑢𝑝𝑑𝑎𝑡𝑒 (𝑖. 𝑒. , 𝑤𝑒𝑖𝑔ℎ𝑡𝑠 𝑎𝑛𝑑 𝑏𝑖𝑎𝑠𝑒𝑠) and 𝜇 ∈ [0,1] is momentum decay rate ( it\n",
        "should be provided as input to the update_parameter method).\n",
        "𝑣 is initially zero. In your initialize_parameters method, you should define a 𝑣 variable for each\n",
        "parameter (i.e., weights and biases) with the same shape as the parameter and set it to zero.\n",
        "Try re-training your neural network with different values of 𝜇 ( e.g., 0.9, 0.95, and 0.99) and interpret\n",
        "the learning curves.\n",
        "Answer the following question:\n",
        "• Does Gradient Descent with Nesterov Momentum help improve your model ?"
      ],
      "metadata": {
        "id": "XcX2Qgxi-PPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(nx, nh):\n",
        "    parameters = {}\n",
        "    velocities = {}  # To store the velocities for Nesterov Momentum\n",
        "\n",
        "    # Initialize weights and biases for the first layer (input to the first hidden layer)\n",
        "    parameters['W1'] = tf.Variable(tf.random.uniform(shape=(nh[0], nx), minval=-0.01, maxval=0.01))\n",
        "    parameters['b1'] = tf.Variable(tf.zeros((nh[0], 1)))\n",
        "\n",
        "    # Initialize velocities (set to zero)\n",
        "    velocities['vW1'] = tf.Variable(tf.zeros((nh[0], nx)))\n",
        "    velocities['vb1'] = tf.Variable(tf.zeros((nh[0], 1)))\n",
        "\n",
        "    for i in range(1, len(nh)):\n",
        "        # Initialize weights and biases for hidden layers\n",
        "        parameters['W' + str(i + 1)] = tf.Variable(tf.random.uniform(shape=(nh[i], nh[i - 1]), minval=-0.01, maxval=0.01))\n",
        "        parameters['b' + str(i + 1)] = tf.Variable(tf.zeros((nh[i], 1)))\n",
        "\n",
        "        # Initialize velocities (set to zero)\n",
        "        velocities['vW' + str(i + 1)] = tf.Variable(tf.zeros((nh[i], nh[i - 1])))\n",
        "        velocities['vb' + str(i + 1)] = tf.Variable(tf.zeros((nh[i], 1)))\n",
        "\n",
        "    return parameters, velocities\n"
      ],
      "metadata": {
        "id": "tNVlAvvt-Pg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters_velocities(parameters, velocities, gradients, learning_rate, mu):\n",
        "    # Update parameters using Nesterov momentum\n",
        "    for i,key in enumerate(parameters.keys()):\n",
        "        v = velocities['v' + key]  # Current velocity\n",
        "        g = gradients[i]  # Gradient with respect to the current parameter\n",
        "\n",
        "        # Update velocity\n",
        "        v.assign(mu * v - learning_rate * g)\n",
        "\n",
        "        # Update parameters using Nesterov update rule\n",
        "        # parameter = parameter + mu * v - learning_rate * g\n",
        "        parameters[key].assign_add(mu * v - learning_rate * g)\n",
        "\n",
        "    return parameters, velocities\n"
      ],
      "metadata": {
        "id": "NWWmbv1kwRjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Create neural network using mini-batch gradient descent with Nesterov momentum**"
      ],
      "metadata": {
        "id": "hWzLQHsf4tVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the neural network with mini-batch gradient descent and Nesterov momentum\n",
        "def create_nn_nesterov(train_X, train_Y, val_X, val_Y, nh, learning_rate, iterations, batch_size, mu):\n",
        "    # Initialize parameters and velocities\n",
        "    parameters, velocities = initialize_parameters(train_X.shape[0], nh)\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        mini_batches = create_mini_batches(train_X, train_Y, batch_size)\n",
        "        epoch_train_losses = []\n",
        "\n",
        "        for (mini_batch_X, mini_batch_Y) in mini_batches:\n",
        "            # Compute train loss and gradients for each mini-batch\n",
        "            with tf.GradientTape() as tape:\n",
        "                yhat_train = forward_pass(parameters, mini_batch_X)\n",
        "                train_loss = compute_loss(mini_batch_Y, yhat_train)\n",
        "\n",
        "            # Record mini-batch loss\n",
        "            epoch_train_losses.append(train_loss.numpy())\n",
        "\n",
        "            # Compute gradients and update parameters using Nesterov momentum\n",
        "            gradients = compute_gradients(train_loss, parameters, tape)\n",
        "            parameters, velocities = update_parameters_velocities(parameters, velocities, gradients, learning_rate, mu)\n",
        "\n",
        "        # Average the loss over all mini-batches for the epoch\n",
        "        avg_train_loss = np.mean(epoch_train_losses)\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "\n",
        "        # Compute validation loss after each epoch\n",
        "        yhat_val = forward_pass(parameters, val_X)\n",
        "        val_loss = compute_loss(val_Y, yhat_val)\n",
        "        val_loss_history.append(val_loss.numpy())\n",
        "\n",
        "        print(f\"Iteration {i}: Avg Train Loss: {avg_train_loss}, Validation Loss: {val_loss}\")\n",
        "\n",
        "    return parameters, train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "04UXmT0gxjSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retraining with different values of mu(0.9,0.95,0.99)\n",
        "parameters, train_history, val_history = create_nn_nesterov(X_train_T, Y_train_onehot_T, X_val_T, Y_val_onehot_T, [512,256,128,10], 0.01, 100, 32, mu=0.9)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2tg0Tmi1PQf",
        "outputId": "c9f9cc44-a43e-466b-da63-fd30f3aeac05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Avg Train Loss: 2.303270101547241, Validation Loss: 2.3023431301116943\n",
            "Iteration 1: Avg Train Loss: 2.30326509475708, Validation Loss: 2.3029685020446777\n",
            "Iteration 2: Avg Train Loss: 2.303194999694824, Validation Loss: 2.3030550479888916\n",
            "Iteration 3: Avg Train Loss: 2.3030266761779785, Validation Loss: 2.303175926208496\n",
            "Iteration 4: Avg Train Loss: 2.3030989170074463, Validation Loss: 2.3028829097747803\n",
            "Iteration 5: Avg Train Loss: 2.3032822608947754, Validation Loss: 2.303347110748291\n",
            "Iteration 6: Avg Train Loss: 2.3031845092773438, Validation Loss: 2.3028650283813477\n",
            "Iteration 7: Avg Train Loss: 2.303176164627075, Validation Loss: 2.302734851837158\n",
            "Iteration 8: Avg Train Loss: 2.3032619953155518, Validation Loss: 2.3027756214141846\n",
            "Iteration 9: Avg Train Loss: 2.3033432960510254, Validation Loss: 2.3023765087127686\n",
            "Iteration 10: Avg Train Loss: 2.3032283782958984, Validation Loss: 2.302560329437256\n",
            "Iteration 11: Avg Train Loss: 2.3030993938446045, Validation Loss: 2.3032336235046387\n",
            "Iteration 12: Avg Train Loss: 2.30301833152771, Validation Loss: 2.302574634552002\n",
            "Iteration 13: Avg Train Loss: 2.303177833557129, Validation Loss: 2.302842855453491\n",
            "Iteration 14: Avg Train Loss: 2.303030014038086, Validation Loss: 2.302332639694214\n",
            "Iteration 15: Avg Train Loss: 2.303158760070801, Validation Loss: 2.3029286861419678\n",
            "Iteration 16: Avg Train Loss: 2.3028500080108643, Validation Loss: 2.302400827407837\n",
            "Iteration 17: Avg Train Loss: 2.3027615547180176, Validation Loss: 2.301891803741455\n",
            "Iteration 18: Avg Train Loss: 2.3019611835479736, Validation Loss: 2.301028251647949\n",
            "Iteration 19: Avg Train Loss: 2.299572706222534, Validation Loss: 2.2929444313049316\n",
            "Iteration 20: Avg Train Loss: 2.2381601333618164, Validation Loss: 2.058701992034912\n",
            "Iteration 21: Avg Train Loss: 1.9311987161636353, Validation Loss: 1.7389554977416992\n",
            "Iteration 22: Avg Train Loss: 1.6675442457199097, Validation Loss: 1.6072313785552979\n",
            "Iteration 23: Avg Train Loss: 1.502724289894104, Validation Loss: 1.420611023902893\n",
            "Iteration 24: Avg Train Loss: 1.3141683340072632, Validation Loss: 1.1991747617721558\n",
            "Iteration 25: Avg Train Loss: 1.1213020086288452, Validation Loss: 1.0236468315124512\n",
            "Iteration 26: Avg Train Loss: 0.9070466160774231, Validation Loss: 0.7846004366874695\n",
            "Iteration 27: Avg Train Loss: 0.6600742936134338, Validation Loss: 0.7138671875\n",
            "Iteration 28: Avg Train Loss: 0.5004743933677673, Validation Loss: 0.5894182324409485\n",
            "Iteration 29: Avg Train Loss: 0.34129735827445984, Validation Loss: 0.5899690389633179\n",
            "Iteration 30: Avg Train Loss: 0.24978423118591309, Validation Loss: 0.49645599722862244\n",
            "Iteration 31: Avg Train Loss: 0.1630350798368454, Validation Loss: 0.537783682346344\n",
            "Iteration 32: Avg Train Loss: 0.11584457010030746, Validation Loss: 0.49695271253585815\n",
            "Iteration 33: Avg Train Loss: 0.08181709796190262, Validation Loss: 0.4766540229320526\n",
            "Iteration 34: Avg Train Loss: 0.08498925715684891, Validation Loss: 0.4949890673160553\n",
            "Iteration 35: Avg Train Loss: 0.05076250806450844, Validation Loss: 0.5087395906448364\n",
            "Iteration 36: Avg Train Loss: 0.04705806076526642, Validation Loss: 0.5193808078765869\n",
            "Iteration 37: Avg Train Loss: 0.03915954753756523, Validation Loss: 0.5307262539863586\n",
            "Iteration 38: Avg Train Loss: 0.052884411066770554, Validation Loss: 0.5406152606010437\n",
            "Iteration 39: Avg Train Loss: 0.04048743098974228, Validation Loss: 0.6182873845100403\n",
            "Iteration 40: Avg Train Loss: 0.05901853367686272, Validation Loss: 0.6367568969726562\n",
            "Iteration 41: Avg Train Loss: 0.08147352188825607, Validation Loss: 0.5717413425445557\n",
            "Iteration 42: Avg Train Loss: 0.05763836205005646, Validation Loss: 0.4888022840023041\n",
            "Iteration 43: Avg Train Loss: 0.03731010481715202, Validation Loss: 0.5971360206604004\n",
            "Iteration 44: Avg Train Loss: 0.023371895775198936, Validation Loss: 0.5290255546569824\n",
            "Iteration 45: Avg Train Loss: 0.025172336027026176, Validation Loss: 0.5267975926399231\n",
            "Iteration 46: Avg Train Loss: 0.01796048693358898, Validation Loss: 0.5437189936637878\n",
            "Iteration 47: Avg Train Loss: 0.0036449157632887363, Validation Loss: 0.5116851925849915\n",
            "Iteration 48: Avg Train Loss: 0.001207422697916627, Validation Loss: 0.5197540521621704\n",
            "Iteration 49: Avg Train Loss: 0.00087381323100999, Validation Loss: 0.5274258255958557\n",
            "Iteration 50: Avg Train Loss: 0.0007407421362586319, Validation Loss: 0.5340254306793213\n",
            "Iteration 51: Avg Train Loss: 0.0006532777915708721, Validation Loss: 0.5393725633621216\n",
            "Iteration 52: Avg Train Loss: 0.0005880967946723104, Validation Loss: 0.5437912344932556\n",
            "Iteration 53: Avg Train Loss: 0.0005378514179028571, Validation Loss: 0.5479762554168701\n",
            "Iteration 54: Avg Train Loss: 0.0004954056348651648, Validation Loss: 0.5520667433738708\n",
            "Iteration 55: Avg Train Loss: 0.00046633247984573245, Validation Loss: 0.5555734634399414\n",
            "Iteration 56: Avg Train Loss: 0.0004321096057537943, Validation Loss: 0.5587494969367981\n",
            "Iteration 57: Avg Train Loss: 0.0004063092637807131, Validation Loss: 0.5616903305053711\n",
            "Iteration 58: Avg Train Loss: 0.00038457204937003553, Validation Loss: 0.5647714138031006\n",
            "Iteration 59: Avg Train Loss: 0.00036508747143670917, Validation Loss: 0.5670217871665955\n",
            "Iteration 60: Avg Train Loss: 0.0003472987737040967, Validation Loss: 0.5698318481445312\n",
            "Iteration 61: Avg Train Loss: 0.00032849388662725687, Validation Loss: 0.5721060037612915\n",
            "Iteration 62: Avg Train Loss: 0.0003135020670015365, Validation Loss: 0.5741544961929321\n",
            "Iteration 63: Avg Train Loss: 0.00030045132734812796, Validation Loss: 0.5762183666229248\n",
            "Iteration 64: Avg Train Loss: 0.00028881264734081924, Validation Loss: 0.5784163475036621\n",
            "Iteration 65: Avg Train Loss: 0.00027768866857513785, Validation Loss: 0.5801609754562378\n",
            "Iteration 66: Avg Train Loss: 0.000267831637756899, Validation Loss: 0.5818915367126465\n",
            "Iteration 67: Avg Train Loss: 0.00025749451015144587, Validation Loss: 0.5833942294120789\n",
            "Iteration 68: Avg Train Loss: 0.0002483822463545948, Validation Loss: 0.5852383971214294\n",
            "Iteration 69: Avg Train Loss: 0.00024009592016227543, Validation Loss: 0.586654007434845\n",
            "Iteration 70: Avg Train Loss: 0.00023251627862919122, Validation Loss: 0.5883487462997437\n",
            "Iteration 71: Avg Train Loss: 0.0002254675782751292, Validation Loss: 0.5897504091262817\n",
            "Iteration 72: Avg Train Loss: 0.00021845675655640662, Validation Loss: 0.5913616418838501\n",
            "Iteration 73: Avg Train Loss: 0.0002131442743120715, Validation Loss: 0.5928492546081543\n",
            "Iteration 74: Avg Train Loss: 0.00020589711493812501, Validation Loss: 0.5939721465110779\n",
            "Iteration 75: Avg Train Loss: 0.0002006479917326942, Validation Loss: 0.5950862765312195\n",
            "Iteration 76: Avg Train Loss: 0.00019527868425939232, Validation Loss: 0.5963001251220703\n",
            "Iteration 77: Avg Train Loss: 0.00019006578077096492, Validation Loss: 0.5974506735801697\n",
            "Iteration 78: Avg Train Loss: 0.00018542450561653823, Validation Loss: 0.5986510515213013\n",
            "Iteration 79: Avg Train Loss: 0.00018142376211471856, Validation Loss: 0.5997647047042847\n",
            "Iteration 80: Avg Train Loss: 0.0001778951846063137, Validation Loss: 0.6009057760238647\n",
            "Iteration 81: Avg Train Loss: 0.00017300565377809107, Validation Loss: 0.6021549701690674\n",
            "Iteration 82: Avg Train Loss: 0.0001684223097981885, Validation Loss: 0.6031738519668579\n",
            "Iteration 83: Avg Train Loss: 0.00016435534053016454, Validation Loss: 0.6041874885559082\n",
            "Iteration 84: Avg Train Loss: 0.00016060408961493522, Validation Loss: 0.605209469795227\n",
            "Iteration 85: Avg Train Loss: 0.00015721986710559577, Validation Loss: 0.6061525344848633\n",
            "Iteration 86: Avg Train Loss: 0.00015414672088809311, Validation Loss: 0.6072156429290771\n",
            "Iteration 87: Avg Train Loss: 0.0001506785920355469, Validation Loss: 0.6081503033638\n",
            "Iteration 88: Avg Train Loss: 0.00014802660734858364, Validation Loss: 0.6090186238288879\n",
            "Iteration 89: Avg Train Loss: 0.00014477573859039694, Validation Loss: 0.6098863482475281\n",
            "Iteration 90: Avg Train Loss: 0.00014198926510289311, Validation Loss: 0.6108259558677673\n",
            "Iteration 91: Avg Train Loss: 0.00013942948135081679, Validation Loss: 0.611534833908081\n",
            "Iteration 92: Avg Train Loss: 0.000137344904942438, Validation Loss: 0.6124838590621948\n",
            "Iteration 93: Avg Train Loss: 0.00013407389633357525, Validation Loss: 0.6133321523666382\n",
            "Iteration 94: Avg Train Loss: 0.00013162405230104923, Validation Loss: 0.6140471696853638\n",
            "Iteration 95: Avg Train Loss: 0.00012929739023093134, Validation Loss: 0.6149340271949768\n",
            "Iteration 96: Avg Train Loss: 0.00012687254638876766, Validation Loss: 0.6156607866287231\n",
            "Iteration 97: Avg Train Loss: 0.00012469457578845322, Validation Loss: 0.6163568496704102\n",
            "Iteration 98: Avg Train Loss: 0.00012305568088777363, Validation Loss: 0.617152750492096\n",
            "Iteration 99: Avg Train Loss: 0.00012066498311469331, Validation Loss: 0.617861270904541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With mu=0.9 (nesterov momentum)\n",
        "\n",
        "Training loss at 100th epoch is 0%\n",
        "\n",
        "Validation Loss: 61.78%"
      ],
      "metadata": {
        "id": "0Y86yGWUE_YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=100\n",
        "plt.plot(range(0,iterations),train_history,'b')\n",
        "plt.plot(range(0,iterations),val_history,'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "KO6pUU_MB5bA",
        "outputId": "c6a3d81f-fca2-4842-d204-4530f4fb1a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, train_history, val_history = create_nn_nesterov(X_train_T, Y_train_onehot_T, X_val_T, Y_val_onehot_T, [512,256,128,10], 0.01, 100, 32, mu=0.95)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qw1hmJ48aTk",
        "outputId": "3754e9b7-dbf1-47f2-e953-2540614fa5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Avg Train Loss: 2.304382562637329, Validation Loss: 2.301478624343872\n",
            "Iteration 1: Avg Train Loss: 2.304100513458252, Validation Loss: 2.302311420440674\n",
            "Iteration 2: Avg Train Loss: 2.3038241863250732, Validation Loss: 2.3028249740600586\n",
            "Iteration 3: Avg Train Loss: 2.3040525913238525, Validation Loss: 2.3021247386932373\n",
            "Iteration 4: Avg Train Loss: 2.304029703140259, Validation Loss: 2.302335262298584\n",
            "Iteration 5: Avg Train Loss: 2.303986072540283, Validation Loss: 2.3026602268218994\n",
            "Iteration 6: Avg Train Loss: 2.3039228916168213, Validation Loss: 2.302624464035034\n",
            "Iteration 7: Avg Train Loss: 2.3038313388824463, Validation Loss: 2.302823781967163\n",
            "Iteration 8: Avg Train Loss: 2.3035330772399902, Validation Loss: 2.301973342895508\n",
            "Iteration 9: Avg Train Loss: 2.3028650283813477, Validation Loss: 2.301985740661621\n",
            "Iteration 10: Avg Train Loss: 2.299654483795166, Validation Loss: 2.289870262145996\n",
            "Iteration 11: Avg Train Loss: 2.0773589611053467, Validation Loss: 1.7147055864334106\n",
            "Iteration 12: Avg Train Loss: 1.6125831604003906, Validation Loss: 1.5374739170074463\n",
            "Iteration 13: Avg Train Loss: 1.219525694847107, Validation Loss: 0.8598482608795166\n",
            "Iteration 14: Avg Train Loss: 0.7231035232543945, Validation Loss: 0.559609055519104\n",
            "Iteration 15: Avg Train Loss: 0.42809101939201355, Validation Loss: 0.42262282967567444\n",
            "Iteration 16: Avg Train Loss: 0.28725582361221313, Validation Loss: 0.45746469497680664\n",
            "Iteration 17: Avg Train Loss: 0.19537761807441711, Validation Loss: 0.4175960421562195\n",
            "Iteration 18: Avg Train Loss: 0.1751721203327179, Validation Loss: 0.3710000216960907\n",
            "Iteration 19: Avg Train Loss: 0.11372160911560059, Validation Loss: 0.3719831109046936\n",
            "Iteration 20: Avg Train Loss: 0.09706924110651016, Validation Loss: 0.3996334969997406\n",
            "Iteration 21: Avg Train Loss: 0.08614546060562134, Validation Loss: 0.34445393085479736\n",
            "Iteration 22: Avg Train Loss: 0.06712052971124649, Validation Loss: 0.4267473816871643\n",
            "Iteration 23: Avg Train Loss: 0.06748461723327637, Validation Loss: 0.531952977180481\n",
            "Iteration 24: Avg Train Loss: 0.10609452426433563, Validation Loss: 0.48113298416137695\n",
            "Iteration 25: Avg Train Loss: 0.07548218965530396, Validation Loss: 0.6168572902679443\n",
            "Iteration 26: Avg Train Loss: 0.05402963608503342, Validation Loss: 0.5834544897079468\n",
            "Iteration 27: Avg Train Loss: 0.050709959119558334, Validation Loss: 0.5024276375770569\n",
            "Iteration 28: Avg Train Loss: 0.027797438204288483, Validation Loss: 0.41899293661117554\n",
            "Iteration 29: Avg Train Loss: 0.020865805447101593, Validation Loss: 0.3872116208076477\n",
            "Iteration 30: Avg Train Loss: 0.024630658328533173, Validation Loss: 0.4917565882205963\n",
            "Iteration 31: Avg Train Loss: 0.004723322112113237, Validation Loss: 0.4355035424232483\n",
            "Iteration 32: Avg Train Loss: 0.013435015454888344, Validation Loss: 0.5322155356407166\n",
            "Iteration 33: Avg Train Loss: 0.029771648347377777, Validation Loss: 0.6030942797660828\n",
            "Iteration 34: Avg Train Loss: 0.04727446660399437, Validation Loss: 0.5991889238357544\n",
            "Iteration 35: Avg Train Loss: 0.04149480164051056, Validation Loss: 0.49103638529777527\n",
            "Iteration 36: Avg Train Loss: 0.006633375305682421, Validation Loss: 0.3987201750278473\n",
            "Iteration 37: Avg Train Loss: 0.008634867146611214, Validation Loss: 0.46467629075050354\n",
            "Iteration 38: Avg Train Loss: 0.008365894667804241, Validation Loss: 0.4288956820964813\n",
            "Iteration 39: Avg Train Loss: 0.0013068438274785876, Validation Loss: 0.42493611574172974\n",
            "Iteration 40: Avg Train Loss: 0.0004151131142862141, Validation Loss: 0.4238250255584717\n",
            "Iteration 41: Avg Train Loss: 0.00023242739553097636, Validation Loss: 0.4284873604774475\n",
            "Iteration 42: Avg Train Loss: 0.00019877433078363538, Validation Loss: 0.4302368760108948\n",
            "Iteration 43: Avg Train Loss: 0.00017180114809889346, Validation Loss: 0.43295273184776306\n",
            "Iteration 44: Avg Train Loss: 0.0001531891175545752, Validation Loss: 0.43518418073654175\n",
            "Iteration 45: Avg Train Loss: 0.00013938400661572814, Validation Loss: 0.4372989237308502\n",
            "Iteration 46: Avg Train Loss: 0.00012834220251534134, Validation Loss: 0.43900391459465027\n",
            "Iteration 47: Avg Train Loss: 0.00011862909013871104, Validation Loss: 0.44103339314460754\n",
            "Iteration 48: Avg Train Loss: 0.00011060563701903448, Validation Loss: 0.44293129444122314\n",
            "Iteration 49: Avg Train Loss: 0.00010349184594815597, Validation Loss: 0.44426023960113525\n",
            "Iteration 50: Avg Train Loss: 9.8050368251279e-05, Validation Loss: 0.445795476436615\n",
            "Iteration 51: Avg Train Loss: 9.248802234651521e-05, Validation Loss: 0.4473477005958557\n",
            "Iteration 52: Avg Train Loss: 8.721688209334388e-05, Validation Loss: 0.44860905408859253\n",
            "Iteration 53: Avg Train Loss: 8.28026095405221e-05, Validation Loss: 0.4496837258338928\n",
            "Iteration 54: Avg Train Loss: 8.032331970753148e-05, Validation Loss: 0.45098450779914856\n",
            "Iteration 55: Avg Train Loss: 7.54018547013402e-05, Validation Loss: 0.45219722390174866\n",
            "Iteration 56: Avg Train Loss: 7.210360490716994e-05, Validation Loss: 0.4531743824481964\n",
            "Iteration 57: Avg Train Loss: 6.931367533979937e-05, Validation Loss: 0.4541076719760895\n",
            "Iteration 58: Avg Train Loss: 6.62945894873701e-05, Validation Loss: 0.4551776945590973\n",
            "Iteration 59: Avg Train Loss: 6.398215191438794e-05, Validation Loss: 0.45609161257743835\n",
            "Iteration 60: Avg Train Loss: 6.150358967715874e-05, Validation Loss: 0.4570263624191284\n",
            "Iteration 61: Avg Train Loss: 5.999251516186632e-05, Validation Loss: 0.4578017294406891\n",
            "Iteration 62: Avg Train Loss: 5.746056922362186e-05, Validation Loss: 0.4587923288345337\n",
            "Iteration 63: Avg Train Loss: 5.653059997712262e-05, Validation Loss: 0.4593399167060852\n",
            "Iteration 64: Avg Train Loss: 5.3701467550126836e-05, Validation Loss: 0.4601152539253235\n",
            "Iteration 65: Avg Train Loss: 5.2053917897865176e-05, Validation Loss: 0.46103060245513916\n",
            "Iteration 66: Avg Train Loss: 5.039403185946867e-05, Validation Loss: 0.4617564380168915\n",
            "Iteration 67: Avg Train Loss: 4.897122198599391e-05, Validation Loss: 0.4623505473136902\n",
            "Iteration 68: Avg Train Loss: 4.7546309360768646e-05, Validation Loss: 0.46303585171699524\n",
            "Iteration 69: Avg Train Loss: 4.6333800128195435e-05, Validation Loss: 0.4635137915611267\n",
            "Iteration 70: Avg Train Loss: 4.534711115411483e-05, Validation Loss: 0.46422815322875977\n",
            "Iteration 71: Avg Train Loss: 4.389305831864476e-05, Validation Loss: 0.4648430049419403\n",
            "Iteration 72: Avg Train Loss: 4.277847256162204e-05, Validation Loss: 0.4654688537120819\n",
            "Iteration 73: Avg Train Loss: 4.1969033190980554e-05, Validation Loss: 0.4658811390399933\n",
            "Iteration 74: Avg Train Loss: 4.075910328538157e-05, Validation Loss: 0.466511994600296\n",
            "Iteration 75: Avg Train Loss: 3.9833405026001856e-05, Validation Loss: 0.4671420454978943\n",
            "Iteration 76: Avg Train Loss: 3.8851248973514885e-05, Validation Loss: 0.4676312506198883\n",
            "Iteration 77: Avg Train Loss: 3.797587851295248e-05, Validation Loss: 0.4680503308773041\n",
            "Iteration 78: Avg Train Loss: 3.74017508875113e-05, Validation Loss: 0.46852222084999084\n",
            "Iteration 79: Avg Train Loss: 3.631141589721665e-05, Validation Loss: 0.46900057792663574\n",
            "Iteration 80: Avg Train Loss: 3.557737363735214e-05, Validation Loss: 0.4695298969745636\n",
            "Iteration 81: Avg Train Loss: 3.5020602808799595e-05, Validation Loss: 0.46997424960136414\n",
            "Iteration 82: Avg Train Loss: 3.41096892952919e-05, Validation Loss: 0.4704093933105469\n",
            "Iteration 83: Avg Train Loss: 3.348377867951058e-05, Validation Loss: 0.4708024561405182\n",
            "Iteration 84: Avg Train Loss: 3.311738691991195e-05, Validation Loss: 0.47121861577033997\n",
            "Iteration 85: Avg Train Loss: 3.248671782785095e-05, Validation Loss: 0.4716903567314148\n",
            "Iteration 86: Avg Train Loss: 3.156622915412299e-05, Validation Loss: 0.47207632660865784\n",
            "Iteration 87: Avg Train Loss: 3.099720925092697e-05, Validation Loss: 0.47246044874191284\n",
            "Iteration 88: Avg Train Loss: 3.0497583793476224e-05, Validation Loss: 0.4728816747665405\n",
            "Iteration 89: Avg Train Loss: 2.995692011609208e-05, Validation Loss: 0.4732823073863983\n",
            "Iteration 90: Avg Train Loss: 2.9375813028309494e-05, Validation Loss: 0.4736306965351105\n",
            "Iteration 91: Avg Train Loss: 2.8871256290585734e-05, Validation Loss: 0.4740096926689148\n",
            "Iteration 92: Avg Train Loss: 2.83701501757605e-05, Validation Loss: 0.4743391275405884\n",
            "Iteration 93: Avg Train Loss: 2.7913192752748728e-05, Validation Loss: 0.4747297167778015\n",
            "Iteration 94: Avg Train Loss: 2.749443956417963e-05, Validation Loss: 0.4751036465167999\n",
            "Iteration 95: Avg Train Loss: 2.7063033485319465e-05, Validation Loss: 0.4754776358604431\n",
            "Iteration 96: Avg Train Loss: 2.6661229640012607e-05, Validation Loss: 0.4757876694202423\n",
            "Iteration 97: Avg Train Loss: 2.6191077267867513e-05, Validation Loss: 0.4761090874671936\n",
            "Iteration 98: Avg Train Loss: 2.5829842343227938e-05, Validation Loss: 0.4762926697731018\n",
            "Iteration 99: Avg Train Loss: 2.54867354669841e-05, Validation Loss: 0.47666892409324646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retraining With gradient descnet of nesterov momentum=0.95:\n",
        "\n",
        "training loss is 2.5e-05 =0.00%\n",
        "\n",
        "val_loss: 0.4769=47.69%\n",
        "\n",
        "When compared with trained model at mu=0.9, val_loss decreased from 67 % to 47.5 with higher momentum value"
      ],
      "metadata": {
        "id": "sl3H8uNUFyqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=100\n",
        "plt.plot(range(0,iterations),train_history,'b')\n",
        "plt.plot(range(0,iterations),val_history,'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "gs34Q1J1B67p",
        "outputId": "7ca5d4e2-45a3-4467-9ae8-8993b07665e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, train_history, val_history = create_nn_nesterov(X_train_T, Y_train_onehot_T, X_val_T, Y_val_onehot_T, [512,256,128,10], 0.01, 100, 32, mu=0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N45JNX36B2mk",
        "outputId": "54ece438-4b95-4d51-8a05-654bc05e1e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Avg Train Loss: 2.3060641288757324, Validation Loss: 2.305968999862671\n",
            "Iteration 1: Avg Train Loss: 2.307396173477173, Validation Loss: 2.302996873855591\n",
            "Iteration 2: Avg Train Loss: 2.307154655456543, Validation Loss: 2.304307699203491\n",
            "Iteration 3: Avg Train Loss: 2.3068363666534424, Validation Loss: 2.302562952041626\n",
            "Iteration 4: Avg Train Loss: 2.2333738803863525, Validation Loss: 1.9191988706588745\n",
            "Iteration 5: Avg Train Loss: 1.6480505466461182, Validation Loss: 1.3150274753570557\n",
            "Iteration 6: Avg Train Loss: 1.0364594459533691, Validation Loss: 0.9204198718070984\n",
            "Iteration 7: Avg Train Loss: 0.6185063123703003, Validation Loss: 0.6422734260559082\n",
            "Iteration 8: Avg Train Loss: 0.5050686597824097, Validation Loss: 0.636395275592804\n",
            "Iteration 9: Avg Train Loss: 0.4238060414791107, Validation Loss: 0.47102591395378113\n",
            "Iteration 10: Avg Train Loss: 0.3406403660774231, Validation Loss: 0.45993348956108093\n",
            "Iteration 11: Avg Train Loss: 0.31324413418769836, Validation Loss: 0.5397671461105347\n",
            "Iteration 12: Avg Train Loss: 0.3332652747631073, Validation Loss: 0.5529660582542419\n",
            "Iteration 13: Avg Train Loss: 0.28497469425201416, Validation Loss: 0.5016447901725769\n",
            "Iteration 14: Avg Train Loss: 0.3135240972042084, Validation Loss: 0.6026210188865662\n",
            "Iteration 15: Avg Train Loss: 0.26557815074920654, Validation Loss: 0.7219551205635071\n",
            "Iteration 16: Avg Train Loss: 0.4890943169593811, Validation Loss: 0.7738350033760071\n",
            "Iteration 17: Avg Train Loss: 0.3833860158920288, Validation Loss: 0.9265315532684326\n",
            "Iteration 18: Avg Train Loss: 0.6938852071762085, Validation Loss: 1.1482785940170288\n",
            "Iteration 19: Avg Train Loss: 0.8498473763465881, Validation Loss: 1.1417359113693237\n",
            "Iteration 20: Avg Train Loss: 1.793127417564392, Validation Loss: 1.9897292852401733\n",
            "Iteration 21: Avg Train Loss: 2.320775270462036, Validation Loss: 2.305931806564331\n",
            "Iteration 22: Avg Train Loss: 2.288210153579712, Validation Loss: 2.4434993267059326\n",
            "Iteration 23: Avg Train Loss: 2.318018913269043, Validation Loss: 2.302159547805786\n",
            "Iteration 24: Avg Train Loss: 2.3098411560058594, Validation Loss: 2.3114116191864014\n",
            "Iteration 25: Avg Train Loss: 2.3076717853546143, Validation Loss: 2.2997591495513916\n",
            "Iteration 26: Avg Train Loss: 2.309957504272461, Validation Loss: 2.30587100982666\n",
            "Iteration 27: Avg Train Loss: 2.30784273147583, Validation Loss: 2.306955337524414\n",
            "Iteration 28: Avg Train Loss: 2.3054039478302, Validation Loss: 2.3017184734344482\n",
            "Iteration 29: Avg Train Loss: 2.3079442977905273, Validation Loss: 2.3096230030059814\n",
            "Iteration 30: Avg Train Loss: 2.307511806488037, Validation Loss: 2.3023085594177246\n",
            "Iteration 31: Avg Train Loss: 2.3082211017608643, Validation Loss: 2.3088173866271973\n",
            "Iteration 32: Avg Train Loss: 2.309603691101074, Validation Loss: 2.302384376525879\n",
            "Iteration 33: Avg Train Loss: 2.3072056770324707, Validation Loss: 2.3067445755004883\n",
            "Iteration 34: Avg Train Loss: 2.308283805847168, Validation Loss: 2.303281784057617\n",
            "Iteration 35: Avg Train Loss: 2.30674409866333, Validation Loss: 2.3072900772094727\n",
            "Iteration 36: Avg Train Loss: 2.3093206882476807, Validation Loss: 2.3015589714050293\n",
            "Iteration 37: Avg Train Loss: 2.311232566833496, Validation Loss: 2.3108201026916504\n",
            "Iteration 38: Avg Train Loss: 2.311739921569824, Validation Loss: 2.305152416229248\n",
            "Iteration 39: Avg Train Loss: 2.3111209869384766, Validation Loss: 2.306110143661499\n",
            "Iteration 40: Avg Train Loss: 2.3064916133880615, Validation Loss: 2.3078017234802246\n",
            "Iteration 41: Avg Train Loss: 2.307706356048584, Validation Loss: 2.304168701171875\n",
            "Iteration 42: Avg Train Loss: 2.3068275451660156, Validation Loss: 2.3047757148742676\n",
            "Iteration 43: Avg Train Loss: 2.3067522048950195, Validation Loss: 2.304626703262329\n",
            "Iteration 44: Avg Train Loss: 2.305382013320923, Validation Loss: 2.305537700653076\n",
            "Iteration 45: Avg Train Loss: 2.3073360919952393, Validation Loss: 2.3069164752960205\n",
            "Iteration 46: Avg Train Loss: 2.306915521621704, Validation Loss: 2.304949998855591\n",
            "Iteration 47: Avg Train Loss: 2.306520938873291, Validation Loss: 2.30171799659729\n",
            "Iteration 48: Avg Train Loss: 2.307002067565918, Validation Loss: 2.3080554008483887\n",
            "Iteration 49: Avg Train Loss: 2.3075647354125977, Validation Loss: 2.301757574081421\n",
            "Iteration 50: Avg Train Loss: 2.306107759475708, Validation Loss: 2.304107666015625\n",
            "Iteration 51: Avg Train Loss: 2.3068277835845947, Validation Loss: 2.3018152713775635\n",
            "Iteration 52: Avg Train Loss: 2.3055996894836426, Validation Loss: 2.3054728507995605\n",
            "Iteration 53: Avg Train Loss: 2.3096513748168945, Validation Loss: 2.3057377338409424\n",
            "Iteration 54: Avg Train Loss: 2.3080642223358154, Validation Loss: 2.301759958267212\n",
            "Iteration 55: Avg Train Loss: 2.308263063430786, Validation Loss: 2.3058621883392334\n",
            "Iteration 56: Avg Train Loss: 2.3082730770111084, Validation Loss: 2.3027100563049316\n",
            "Iteration 57: Avg Train Loss: 2.3102052211761475, Validation Loss: 2.3034675121307373\n",
            "Iteration 58: Avg Train Loss: 2.309666633605957, Validation Loss: 2.3056347370147705\n",
            "Iteration 59: Avg Train Loss: 2.3045382499694824, Validation Loss: 2.3028206825256348\n",
            "Iteration 60: Avg Train Loss: 2.306756019592285, Validation Loss: 2.3061041831970215\n",
            "Iteration 61: Avg Train Loss: 2.30824875831604, Validation Loss: 2.302307367324829\n",
            "Iteration 62: Avg Train Loss: 2.3098223209381104, Validation Loss: 2.309558391571045\n",
            "Iteration 63: Avg Train Loss: 2.3073620796203613, Validation Loss: 2.301870584487915\n",
            "Iteration 64: Avg Train Loss: 2.3065295219421387, Validation Loss: 2.3013288974761963\n",
            "Iteration 65: Avg Train Loss: 2.307783842086792, Validation Loss: 2.3061726093292236\n",
            "Iteration 66: Avg Train Loss: 2.307158946990967, Validation Loss: 2.3088366985321045\n",
            "Iteration 67: Avg Train Loss: 2.309159994125366, Validation Loss: 2.30424165725708\n",
            "Iteration 68: Avg Train Loss: 2.308070659637451, Validation Loss: 2.3084969520568848\n",
            "Iteration 69: Avg Train Loss: 2.3091657161712646, Validation Loss: 2.305361032485962\n",
            "Iteration 70: Avg Train Loss: 2.308427333831787, Validation Loss: 2.306502103805542\n",
            "Iteration 71: Avg Train Loss: 2.3064305782318115, Validation Loss: 2.304344892501831\n",
            "Iteration 72: Avg Train Loss: 2.3090298175811768, Validation Loss: 2.3079628944396973\n",
            "Iteration 73: Avg Train Loss: 2.3089561462402344, Validation Loss: 2.3031723499298096\n",
            "Iteration 74: Avg Train Loss: 2.306748628616333, Validation Loss: 2.3050334453582764\n",
            "Iteration 75: Avg Train Loss: 2.3084022998809814, Validation Loss: 2.307248592376709\n",
            "Iteration 76: Avg Train Loss: 2.307440996170044, Validation Loss: 2.3052821159362793\n",
            "Iteration 77: Avg Train Loss: 2.306701421737671, Validation Loss: 2.3054935932159424\n",
            "Iteration 78: Avg Train Loss: 2.3067963123321533, Validation Loss: 2.3023693561553955\n",
            "Iteration 79: Avg Train Loss: 2.3086087703704834, Validation Loss: 2.305640697479248\n",
            "Iteration 80: Avg Train Loss: 2.307441234588623, Validation Loss: 2.3069562911987305\n",
            "Iteration 81: Avg Train Loss: 2.309750556945801, Validation Loss: 2.302110195159912\n",
            "Iteration 82: Avg Train Loss: 2.3070080280303955, Validation Loss: 2.3052942752838135\n",
            "Iteration 83: Avg Train Loss: 2.307929754257202, Validation Loss: 2.3037312030792236\n",
            "Iteration 84: Avg Train Loss: 2.30815052986145, Validation Loss: 2.304166793823242\n",
            "Iteration 85: Avg Train Loss: 2.307696580886841, Validation Loss: 2.303488254547119\n",
            "Iteration 86: Avg Train Loss: 2.307844400405884, Validation Loss: 2.3100926876068115\n",
            "Iteration 87: Avg Train Loss: 2.310091972351074, Validation Loss: 2.3020429611206055\n",
            "Iteration 88: Avg Train Loss: 2.308506488800049, Validation Loss: 2.3081459999084473\n",
            "Iteration 89: Avg Train Loss: 2.3099100589752197, Validation Loss: 2.3086471557617188\n",
            "Iteration 90: Avg Train Loss: 2.3080761432647705, Validation Loss: 2.300753116607666\n",
            "Iteration 91: Avg Train Loss: 2.306347370147705, Validation Loss: 2.3089513778686523\n",
            "Iteration 92: Avg Train Loss: 2.3075530529022217, Validation Loss: 2.3053839206695557\n",
            "Iteration 93: Avg Train Loss: 2.3086020946502686, Validation Loss: 2.3047091960906982\n",
            "Iteration 94: Avg Train Loss: 2.305109739303589, Validation Loss: 2.3030130863189697\n",
            "Iteration 95: Avg Train Loss: 2.3079206943511963, Validation Loss: 2.303013801574707\n",
            "Iteration 96: Avg Train Loss: 2.308741331100464, Validation Loss: 2.3052561283111572\n",
            "Iteration 97: Avg Train Loss: 2.3080337047576904, Validation Loss: 2.3050198554992676\n",
            "Iteration 98: Avg Train Loss: 2.309492349624634, Validation Loss: 2.307681083679199\n",
            "Iteration 99: Avg Train Loss: 2.3092079162597656, Validation Loss: 2.3017783164978027\n",
            "Iteration 100: Avg Train Loss: 2.3072781562805176, Validation Loss: 2.305788516998291\n",
            "Iteration 101: Avg Train Loss: 2.30507230758667, Validation Loss: 2.3031318187713623\n",
            "Iteration 102: Avg Train Loss: 2.308123826980591, Validation Loss: 2.308223009109497\n",
            "Iteration 103: Avg Train Loss: 2.3059818744659424, Validation Loss: 2.3073887825012207\n",
            "Iteration 104: Avg Train Loss: 2.3064587116241455, Validation Loss: 2.2999019622802734\n",
            "Iteration 105: Avg Train Loss: 2.3072402477264404, Validation Loss: 2.306300401687622\n",
            "Iteration 106: Avg Train Loss: 2.3101301193237305, Validation Loss: 2.3028409481048584\n",
            "Iteration 107: Avg Train Loss: 2.308473825454712, Validation Loss: 2.3064658641815186\n",
            "Iteration 108: Avg Train Loss: 2.308116912841797, Validation Loss: 2.3065311908721924\n",
            "Iteration 109: Avg Train Loss: 2.3071601390838623, Validation Loss: 2.3048837184906006\n",
            "Iteration 110: Avg Train Loss: 2.3080108165740967, Validation Loss: 2.3054823875427246\n",
            "Iteration 111: Avg Train Loss: 2.307645797729492, Validation Loss: 2.301832437515259\n",
            "Iteration 112: Avg Train Loss: 2.307910203933716, Validation Loss: 2.3041839599609375\n",
            "Iteration 113: Avg Train Loss: 2.3114631175994873, Validation Loss: 2.3150792121887207\n",
            "Iteration 114: Avg Train Loss: 2.3106496334075928, Validation Loss: 2.3025693893432617\n",
            "Iteration 115: Avg Train Loss: 2.309300422668457, Validation Loss: 2.302932024002075\n",
            "Iteration 116: Avg Train Loss: 2.3077645301818848, Validation Loss: 2.307630777359009\n",
            "Iteration 117: Avg Train Loss: 2.3087666034698486, Validation Loss: 2.30419659614563\n",
            "Iteration 118: Avg Train Loss: 2.3105902671813965, Validation Loss: 2.3035898208618164\n",
            "Iteration 119: Avg Train Loss: 2.3101489543914795, Validation Loss: 2.302050828933716\n",
            "Iteration 120: Avg Train Loss: 2.3076441287994385, Validation Loss: 2.307868003845215\n",
            "Iteration 121: Avg Train Loss: 2.30972957611084, Validation Loss: 2.302211046218872\n",
            "Iteration 122: Avg Train Loss: 2.305473566055298, Validation Loss: 2.309530258178711\n",
            "Iteration 123: Avg Train Loss: 2.3100638389587402, Validation Loss: 2.3042666912078857\n",
            "Iteration 124: Avg Train Loss: 2.308746337890625, Validation Loss: 2.3069634437561035\n",
            "Iteration 125: Avg Train Loss: 2.3066556453704834, Validation Loss: 2.3078901767730713\n",
            "Iteration 126: Avg Train Loss: 2.307481527328491, Validation Loss: 2.303903102874756\n",
            "Iteration 127: Avg Train Loss: 2.307291030883789, Validation Loss: 2.306938409805298\n",
            "Iteration 128: Avg Train Loss: 2.3095126152038574, Validation Loss: 2.3078651428222656\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-1dc2a2ba6db9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_nn_nesterov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_onehot_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val_onehot_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-ef307e4246eb>\u001b[0m in \u001b[0;36mcreate_nn_nesterov\u001b[0;34m(train_X, train_Y, val_X, val_Y, nh, learning_rate, iterations, batch_size, mu)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Compute gradients and update parameters using Nesterov momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters_velocities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Average the loss over all mini-batches for the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-3c4f27145119>\u001b[0m in \u001b[0;36mupdate_parameters_velocities\u001b[0;34m(parameters, velocities, gradients, learning_rate, mu)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Update velocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Update parameters using Nesterov update rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/override_binary_operator.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m_mul_dispatch_factory\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1711\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    524\u001b[0m   \"\"\"\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6822\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6823\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6824\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6825\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[1;32m   6826\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=100\n",
        "plt.plot(range(0,iterations),train_history,'b')\n",
        "plt.plot(range(0,iterations),val_history,'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "VJV3gCwK1Pt7",
        "outputId": "54b60141-dd15-4913-c546-870e7be6ebad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With mu=0.99, losses converged but were higher tahn the earlier losses obtained with other models trained above."
      ],
      "metadata": {
        "id": "X_kmnOhym4mP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " *Gradient Descent with Nesterov Momentum helped in the improvement of  this model in  **terms of number of epochs **taken for training which are as few as a 100 as it just took 500 epochs to get to a loss values reached with a minibatch gradient descent of 500 epochs. *\n",
        "\n",
        " Gradient Descent with Nesterov Momentum of 0.9 and 0.95 resulted inlower losses at fewer epochs."
      ],
      "metadata": {
        "id": "BXuxD9OyD6U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3- Reduce Learning Rate on Plateau\n",
        "Reduce the learning rate by half if no improvement is seen in validation loss for 10 epochs.\n",
        "Should monitor and keep track of the best validation loss and the parameters for the model with that\n",
        "loss. If no improvement is seen in validation loss for 10 epochs, then reduce the learning rate by 0.5\n",
        "and continue the training with that learning rate. Stop training if the learning rate is reduced to\n",
        "minimum value ( e.g., 1e-4) and return the parameters with the lowest validation loss.\n"
      ],
      "metadata": {
        "id": "2lLJiZup-a4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Referred google\n",
        "def create_nn_plateau(train_X, train_Y, val_X, val_Y, nh, initial_lr, iterations, batch_size, mu, min_lr=1e-4, patience=10):\n",
        "    # Initialize parameters and velocities\n",
        "    parameters, velocities = initialize_parameters(train_X.shape[0], nh)\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    best_val_loss = float('inf')  # Best validation loss initialized to infinity\n",
        "    best_parameters = copy.deepcopy(parameters)  # Best parameters initialized as a copy\n",
        "    best_epoch = -1  # Epoch with the best validation loss\n",
        "    lr = initial_lr  # Start with the initial learning rate\n",
        "    plateau_counter = 0  # Counter to track how many epochs without improvement\n",
        "\n",
        "    for i in range(iterations):\n",
        "        mini_batches = create_mini_batches(train_X, train_Y, batch_size)\n",
        "        epoch_train_losses = []\n",
        "\n",
        "        # Train on each mini-batch\n",
        "        for (mini_batch_X, mini_batch_Y) in mini_batches:\n",
        "            # Compute train loss and gradients for each mini-batch\n",
        "            with tf.GradientTape() as tape:\n",
        "                yhat_train = forward_pass(parameters, mini_batch_X)\n",
        "                train_loss = compute_loss(mini_batch_Y, yhat_train)\n",
        "\n",
        "            # Record mini-batch loss\n",
        "            epoch_train_losses.append(train_loss.numpy())\n",
        "\n",
        "            # Compute gradients and update parameters using Nesterov momentum\n",
        "            gradients = compute_gradients(train_loss, parameters, tape)\n",
        "            parameters, velocities = update_parameters_velocities(parameters, velocities, gradients, lr, mu)\n",
        "\n",
        "        # Average the loss over all mini-batches for the epoch\n",
        "        avg_train_loss = np.mean(epoch_train_losses)\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "\n",
        "        # Compute validation loss after each epoch\n",
        "        yhat_val = forward_pass(parameters, val_X)\n",
        "        val_loss = compute_loss(val_Y, yhat_val)\n",
        "        avg_val_loss = val_loss.numpy()\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Iteration {i}: Avg Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Learning Rate: {lr}\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_parameters = copy.deepcopy(parameters)  # Save the best parameters\n",
        "            best_epoch = i\n",
        "            plateau_counter = 0  # Reset plateau counter if there is improvement\n",
        "        else:\n",
        "            plateau_counter += 1\n",
        "\n",
        "        # Reduce learning rate if no improvement for `patience` epochs\n",
        "        if plateau_counter >= patience:\n",
        "            if lr > min_lr:\n",
        "                lr = max(lr * 0.5, min_lr)  # Reduce learning rate by half but ensure it's not below min_lr\n",
        "                print(f\"Reducing learning rate to {lr}\")\n",
        "                plateau_counter = 0  # Reset plateau counter after reducing learning rate\n",
        "            else:\n",
        "                print(f\"Learning rate reduced to minimum value ({min_lr}). Stopping training.\")\n",
        "                break  # Stop training if learning rate reaches minimum value\n",
        "\n",
        "    print(f\"Training completed. Best validation loss: {best_val_loss} at epoch {best_epoch}.\")\n",
        "    return best_parameters, train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "d0QpOjD9-bzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters, train_history, val_history = create_nn_plateau(\n",
        "    X_train_T, Y_train_onehot_T,\n",
        "    X_val_T, Y_val_onehot_T,\n",
        "    [512,256,128,10],\n",
        "    initial_lr=0.01,\n",
        "    iterations=500,\n",
        "    batch_size=32,\n",
        "    mu=0.9,\n",
        "    min_lr=1e-4,\n",
        "    patience=10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRcYE_xhDcpQ",
        "outputId": "fadd0721-40bd-4f29-c686-d40674a9d5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Avg Train Loss: 2.303183078765869, Validation Loss: 2.30212140083313, Learning Rate: 0.01\n",
            "Iteration 1: Avg Train Loss: 2.303197145462036, Validation Loss: 2.3018791675567627, Learning Rate: 0.01\n",
            "Iteration 2: Avg Train Loss: 2.3034470081329346, Validation Loss: 2.3026809692382812, Learning Rate: 0.01\n",
            "Iteration 3: Avg Train Loss: 2.3032634258270264, Validation Loss: 2.3024744987487793, Learning Rate: 0.01\n",
            "Iteration 4: Avg Train Loss: 2.3032782077789307, Validation Loss: 2.3027303218841553, Learning Rate: 0.01\n",
            "Iteration 5: Avg Train Loss: 2.303215265274048, Validation Loss: 2.3028781414031982, Learning Rate: 0.01\n",
            "Iteration 6: Avg Train Loss: 2.303241014480591, Validation Loss: 2.3030645847320557, Learning Rate: 0.01\n",
            "Iteration 7: Avg Train Loss: 2.3033037185668945, Validation Loss: 2.3028738498687744, Learning Rate: 0.01\n",
            "Iteration 8: Avg Train Loss: 2.303256034851074, Validation Loss: 2.3024849891662598, Learning Rate: 0.01\n",
            "Iteration 9: Avg Train Loss: 2.303422212600708, Validation Loss: 2.302424430847168, Learning Rate: 0.01\n",
            "Iteration 10: Avg Train Loss: 2.3030190467834473, Validation Loss: 2.3027408123016357, Learning Rate: 0.01\n",
            "Iteration 11: Avg Train Loss: 2.3031702041625977, Validation Loss: 2.3022682666778564, Learning Rate: 0.01\n",
            "Reducing learning rate to 0.005\n",
            "Iteration 12: Avg Train Loss: 2.302708148956299, Validation Loss: 2.302280902862549, Learning Rate: 0.005\n",
            "Iteration 13: Avg Train Loss: 2.302490711212158, Validation Loss: 2.302579164505005, Learning Rate: 0.005\n",
            "Iteration 14: Avg Train Loss: 2.3026187419891357, Validation Loss: 2.3026936054229736, Learning Rate: 0.005\n",
            "Iteration 15: Avg Train Loss: 2.3026576042175293, Validation Loss: 2.3024988174438477, Learning Rate: 0.005\n",
            "Iteration 16: Avg Train Loss: 2.3024697303771973, Validation Loss: 2.3026187419891357, Learning Rate: 0.005\n",
            "Iteration 17: Avg Train Loss: 2.302492141723633, Validation Loss: 2.302654266357422, Learning Rate: 0.005\n",
            "Iteration 18: Avg Train Loss: 2.3024685382843018, Validation Loss: 2.3023128509521484, Learning Rate: 0.005\n",
            "Iteration 19: Avg Train Loss: 2.3024404048919678, Validation Loss: 2.302341938018799, Learning Rate: 0.005\n",
            "Iteration 20: Avg Train Loss: 2.302441120147705, Validation Loss: 2.302304267883301, Learning Rate: 0.005\n",
            "Iteration 21: Avg Train Loss: 2.302330732345581, Validation Loss: 2.3023433685302734, Learning Rate: 0.005\n",
            "Reducing learning rate to 0.0025\n",
            "Iteration 22: Avg Train Loss: 2.3019051551818848, Validation Loss: 2.302325487136841, Learning Rate: 0.0025\n",
            "Iteration 23: Avg Train Loss: 2.301940679550171, Validation Loss: 2.3022091388702393, Learning Rate: 0.0025\n",
            "Iteration 24: Avg Train Loss: 2.301745653152466, Validation Loss: 2.3020498752593994, Learning Rate: 0.0025\n",
            "Iteration 25: Avg Train Loss: 2.301664352416992, Validation Loss: 2.301785707473755, Learning Rate: 0.0025\n",
            "Iteration 26: Avg Train Loss: 2.3014988899230957, Validation Loss: 2.3014767169952393, Learning Rate: 0.0025\n",
            "Iteration 27: Avg Train Loss: 2.301164150238037, Validation Loss: 2.3010780811309814, Learning Rate: 0.0025\n",
            "Iteration 28: Avg Train Loss: 2.3008763790130615, Validation Loss: 2.3007235527038574, Learning Rate: 0.0025\n",
            "Iteration 29: Avg Train Loss: 2.3003201484680176, Validation Loss: 2.30008602142334, Learning Rate: 0.0025\n",
            "Iteration 30: Avg Train Loss: 2.299471378326416, Validation Loss: 2.2987191677093506, Learning Rate: 0.0025\n",
            "Iteration 31: Avg Train Loss: 2.297943353652954, Validation Loss: 2.296600103378296, Learning Rate: 0.0025\n",
            "Iteration 32: Avg Train Loss: 2.2950832843780518, Validation Loss: 2.2914814949035645, Learning Rate: 0.0025\n",
            "Iteration 33: Avg Train Loss: 2.2877774238586426, Validation Loss: 2.2779500484466553, Learning Rate: 0.0025\n",
            "Iteration 34: Avg Train Loss: 2.263094663619995, Validation Loss: 2.223780870437622, Learning Rate: 0.0025\n",
            "Iteration 35: Avg Train Loss: 2.1727209091186523, Validation Loss: 2.078202247619629, Learning Rate: 0.0025\n",
            "Iteration 36: Avg Train Loss: 2.0439064502716064, Validation Loss: 1.962888240814209, Learning Rate: 0.0025\n",
            "Iteration 37: Avg Train Loss: 1.9346357583999634, Validation Loss: 1.8526028394699097, Learning Rate: 0.0025\n",
            "Iteration 38: Avg Train Loss: 1.812484860420227, Validation Loss: 1.744654893875122, Learning Rate: 0.0025\n",
            "Iteration 39: Avg Train Loss: 1.696770429611206, Validation Loss: 1.64625883102417, Learning Rate: 0.0025\n",
            "Iteration 40: Avg Train Loss: 1.6372545957565308, Validation Loss: 1.592697024345398, Learning Rate: 0.0025\n",
            "Iteration 41: Avg Train Loss: 1.601895809173584, Validation Loss: 1.5596776008605957, Learning Rate: 0.0025\n",
            "Iteration 42: Avg Train Loss: 1.5646231174468994, Validation Loss: 1.5469590425491333, Learning Rate: 0.0025\n",
            "Iteration 43: Avg Train Loss: 1.5358134508132935, Validation Loss: 1.515108346939087, Learning Rate: 0.0025\n",
            "Iteration 44: Avg Train Loss: 1.5066591501235962, Validation Loss: 1.4890140295028687, Learning Rate: 0.0025\n",
            "Iteration 45: Avg Train Loss: 1.4557567834854126, Validation Loss: 1.4683705568313599, Learning Rate: 0.0025\n",
            "Iteration 46: Avg Train Loss: 1.4190056324005127, Validation Loss: 1.4280511140823364, Learning Rate: 0.0025\n",
            "Iteration 47: Avg Train Loss: 1.3713009357452393, Validation Loss: 1.3833457231521606, Learning Rate: 0.0025\n",
            "Iteration 48: Avg Train Loss: 1.312217116355896, Validation Loss: 1.3240548372268677, Learning Rate: 0.0025\n",
            "Iteration 49: Avg Train Loss: 1.2527718544006348, Validation Loss: 1.3025412559509277, Learning Rate: 0.0025\n",
            "Iteration 50: Avg Train Loss: 1.1865326166152954, Validation Loss: 1.2373515367507935, Learning Rate: 0.0025\n",
            "Iteration 51: Avg Train Loss: 1.1170991659164429, Validation Loss: 1.137238621711731, Learning Rate: 0.0025\n",
            "Iteration 52: Avg Train Loss: 1.015100121498108, Validation Loss: 1.0212913751602173, Learning Rate: 0.0025\n",
            "Iteration 53: Avg Train Loss: 0.9049939513206482, Validation Loss: 0.9190342426300049, Learning Rate: 0.0025\n",
            "Iteration 54: Avg Train Loss: 0.7896938920021057, Validation Loss: 0.8567009568214417, Learning Rate: 0.0025\n",
            "Iteration 55: Avg Train Loss: 0.6904441714286804, Validation Loss: 0.7820841073989868, Learning Rate: 0.0025\n",
            "Iteration 56: Avg Train Loss: 0.6121432781219482, Validation Loss: 0.7079719305038452, Learning Rate: 0.0025\n",
            "Iteration 57: Avg Train Loss: 0.5385081171989441, Validation Loss: 0.6737815141677856, Learning Rate: 0.0025\n",
            "Iteration 58: Avg Train Loss: 0.4766191840171814, Validation Loss: 0.6469467282295227, Learning Rate: 0.0025\n",
            "Iteration 59: Avg Train Loss: 0.4215407967567444, Validation Loss: 0.6368036270141602, Learning Rate: 0.0025\n",
            "Iteration 60: Avg Train Loss: 0.3765162527561188, Validation Loss: 0.6436612010002136, Learning Rate: 0.0025\n",
            "Iteration 61: Avg Train Loss: 0.3293757438659668, Validation Loss: 0.5659334063529968, Learning Rate: 0.0025\n",
            "Iteration 62: Avg Train Loss: 0.2826319634914398, Validation Loss: 0.5875659584999084, Learning Rate: 0.0025\n",
            "Iteration 63: Avg Train Loss: 0.24555285274982452, Validation Loss: 0.6007480025291443, Learning Rate: 0.0025\n",
            "Iteration 64: Avg Train Loss: 0.21111010015010834, Validation Loss: 0.5440319180488586, Learning Rate: 0.0025\n",
            "Iteration 65: Avg Train Loss: 0.17935484647750854, Validation Loss: 0.5777869820594788, Learning Rate: 0.0025\n",
            "Iteration 66: Avg Train Loss: 0.1520734429359436, Validation Loss: 0.577020525932312, Learning Rate: 0.0025\n",
            "Iteration 67: Avg Train Loss: 0.12687592208385468, Validation Loss: 0.5962546467781067, Learning Rate: 0.0025\n",
            "Iteration 68: Avg Train Loss: 0.11081495881080627, Validation Loss: 0.6003268957138062, Learning Rate: 0.0025\n",
            "Iteration 69: Avg Train Loss: 0.09075777232646942, Validation Loss: 0.589159369468689, Learning Rate: 0.0025\n",
            "Iteration 70: Avg Train Loss: 0.0725487470626831, Validation Loss: 0.6148476600646973, Learning Rate: 0.0025\n",
            "Iteration 71: Avg Train Loss: 0.06213323399424553, Validation Loss: 0.6151658892631531, Learning Rate: 0.0025\n",
            "Iteration 72: Avg Train Loss: 0.052375249564647675, Validation Loss: 0.6197509169578552, Learning Rate: 0.0025\n",
            "Iteration 73: Avg Train Loss: 0.04321225732564926, Validation Loss: 0.6827569007873535, Learning Rate: 0.0025\n",
            "Iteration 74: Avg Train Loss: 0.04000423476099968, Validation Loss: 0.6417964696884155, Learning Rate: 0.0025\n",
            "Reducing learning rate to 0.00125\n",
            "Iteration 75: Avg Train Loss: 0.03140396997332573, Validation Loss: 0.6518494486808777, Learning Rate: 0.00125\n",
            "Iteration 76: Avg Train Loss: 0.026860207319259644, Validation Loss: 0.649909257888794, Learning Rate: 0.00125\n",
            "Iteration 77: Avg Train Loss: 0.02433459646999836, Validation Loss: 0.6550862789154053, Learning Rate: 0.00125\n",
            "Iteration 78: Avg Train Loss: 0.022860949859023094, Validation Loss: 0.6595827341079712, Learning Rate: 0.00125\n",
            "Iteration 79: Avg Train Loss: 0.02116263471543789, Validation Loss: 0.6693867444992065, Learning Rate: 0.00125\n",
            "Iteration 80: Avg Train Loss: 0.019906828179955482, Validation Loss: 0.6720770001411438, Learning Rate: 0.00125\n",
            "Iteration 81: Avg Train Loss: 0.01889641024172306, Validation Loss: 0.6775849461555481, Learning Rate: 0.00125\n",
            "Iteration 82: Avg Train Loss: 0.01776820421218872, Validation Loss: 0.6842674612998962, Learning Rate: 0.00125\n",
            "Iteration 83: Avg Train Loss: 0.01691300794482231, Validation Loss: 0.6821895241737366, Learning Rate: 0.00125\n",
            "Iteration 84: Avg Train Loss: 0.015983521938323975, Validation Loss: 0.6922383308410645, Learning Rate: 0.00125\n",
            "Reducing learning rate to 0.000625\n",
            "Iteration 85: Avg Train Loss: 0.014919308945536613, Validation Loss: 0.6916162371635437, Learning Rate: 0.000625\n",
            "Iteration 86: Avg Train Loss: 0.014458153396844864, Validation Loss: 0.6949570775032043, Learning Rate: 0.000625\n",
            "Iteration 87: Avg Train Loss: 0.01413260493427515, Validation Loss: 0.6977358460426331, Learning Rate: 0.000625\n",
            "Iteration 88: Avg Train Loss: 0.01377863995730877, Validation Loss: 0.6988433003425598, Learning Rate: 0.000625\n",
            "Iteration 89: Avg Train Loss: 0.013501779176294804, Validation Loss: 0.7032715082168579, Learning Rate: 0.000625\n",
            "Iteration 90: Avg Train Loss: 0.013180444948375225, Validation Loss: 0.7046623826026917, Learning Rate: 0.000625\n",
            "Iteration 91: Avg Train Loss: 0.012902392074465752, Validation Loss: 0.7044675946235657, Learning Rate: 0.000625\n",
            "Iteration 92: Avg Train Loss: 0.0125724570825696, Validation Loss: 0.7069575190544128, Learning Rate: 0.000625\n",
            "Iteration 93: Avg Train Loss: 0.012352471239864826, Validation Loss: 0.709865391254425, Learning Rate: 0.000625\n",
            "Iteration 94: Avg Train Loss: 0.012020088732242584, Validation Loss: 0.7107183933258057, Learning Rate: 0.000625\n",
            "Reducing learning rate to 0.0003125\n",
            "Iteration 95: Avg Train Loss: 0.011804827488958836, Validation Loss: 0.7121061682701111, Learning Rate: 0.0003125\n",
            "Iteration 96: Avg Train Loss: 0.01158338226377964, Validation Loss: 0.7108610272407532, Learning Rate: 0.0003125\n",
            "Iteration 97: Avg Train Loss: 0.011500517837703228, Validation Loss: 0.7145475745201111, Learning Rate: 0.0003125\n",
            "Iteration 98: Avg Train Loss: 0.011345372535288334, Validation Loss: 0.7129843235015869, Learning Rate: 0.0003125\n",
            "Iteration 99: Avg Train Loss: 0.011232390068471432, Validation Loss: 0.714543879032135, Learning Rate: 0.0003125\n",
            "Iteration 100: Avg Train Loss: 0.01116365659981966, Validation Loss: 0.7156630158424377, Learning Rate: 0.0003125\n",
            "Iteration 101: Avg Train Loss: 0.011031489819288254, Validation Loss: 0.7166576385498047, Learning Rate: 0.0003125\n",
            "Iteration 102: Avg Train Loss: 0.01093266811221838, Validation Loss: 0.7177479267120361, Learning Rate: 0.0003125\n",
            "Iteration 103: Avg Train Loss: 0.010831981897354126, Validation Loss: 0.7193785905838013, Learning Rate: 0.0003125\n",
            "Iteration 104: Avg Train Loss: 0.010701535269618034, Validation Loss: 0.7206347584724426, Learning Rate: 0.0003125\n",
            "Reducing learning rate to 0.00015625\n",
            "Iteration 105: Avg Train Loss: 0.010598075576126575, Validation Loss: 0.7193222045898438, Learning Rate: 0.00015625\n",
            "Iteration 106: Avg Train Loss: 0.010520011186599731, Validation Loss: 0.7200665473937988, Learning Rate: 0.00015625\n",
            "Iteration 107: Avg Train Loss: 0.010440945625305176, Validation Loss: 0.7202916741371155, Learning Rate: 0.00015625\n",
            "Iteration 108: Avg Train Loss: 0.010413403622806072, Validation Loss: 0.7208768725395203, Learning Rate: 0.00015625\n",
            "Iteration 109: Avg Train Loss: 0.0103748245164752, Validation Loss: 0.7211905717849731, Learning Rate: 0.00015625\n",
            "Iteration 110: Avg Train Loss: 0.010306211188435555, Validation Loss: 0.7225708365440369, Learning Rate: 0.00015625\n",
            "Iteration 111: Avg Train Loss: 0.010272511281073093, Validation Loss: 0.7218341827392578, Learning Rate: 0.00015625\n",
            "Iteration 112: Avg Train Loss: 0.010229894891381264, Validation Loss: 0.7223920226097107, Learning Rate: 0.00015625\n",
            "Iteration 113: Avg Train Loss: 0.010189640335738659, Validation Loss: 0.7230978012084961, Learning Rate: 0.00015625\n",
            "Iteration 114: Avg Train Loss: 0.010120851919054985, Validation Loss: 0.7233518362045288, Learning Rate: 0.00015625\n",
            "Reducing learning rate to 0.0001\n",
            "Iteration 115: Avg Train Loss: 0.010047094896435738, Validation Loss: 0.7237528562545776, Learning Rate: 0.0001\n",
            "Iteration 116: Avg Train Loss: 0.010161162354052067, Validation Loss: 0.7239446043968201, Learning Rate: 0.0001\n",
            "Iteration 117: Avg Train Loss: 0.009998532012104988, Validation Loss: 0.7239916324615479, Learning Rate: 0.0001\n",
            "Iteration 118: Avg Train Loss: 0.009984748438000679, Validation Loss: 0.7244926691055298, Learning Rate: 0.0001\n",
            "Iteration 119: Avg Train Loss: 0.009935444220900536, Validation Loss: 0.7245272994041443, Learning Rate: 0.0001\n",
            "Iteration 120: Avg Train Loss: 0.00994997937232256, Validation Loss: 0.7249024510383606, Learning Rate: 0.0001\n",
            "Iteration 121: Avg Train Loss: 0.009879284538328648, Validation Loss: 0.7249928116798401, Learning Rate: 0.0001\n",
            "Iteration 122: Avg Train Loss: 0.00985065009444952, Validation Loss: 0.7252532243728638, Learning Rate: 0.0001\n",
            "Iteration 123: Avg Train Loss: 0.009841633960604668, Validation Loss: 0.7252722382545471, Learning Rate: 0.0001\n",
            "Iteration 124: Avg Train Loss: 0.009813123382627964, Validation Loss: 0.7255162000656128, Learning Rate: 0.0001\n",
            "Learning rate reduced to minimum value (0.0001). Stopping training.\n",
            "Training completed. Best validation loss: 0.5440319180488586 at epoch 64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters obtained at the lowest validation loss:\")\n",
        "\n",
        "for param_name, param_value in best_parameters.items():\n",
        "    print(f\"{param_name}: {param_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcoxtdh0TvoZ",
        "outputId": "244c1042-1fef-4393-ad49-7ccff572f6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters obtained at the lowest validation loss:\n",
            "W1: <tf.Variable 'Variable:0' shape=(512, 1600) dtype=float32, numpy=\n",
            "array([[ 3.9666751e-03,  5.7922327e-03,  5.3362604e-03, ...,\n",
            "         1.1658204e-04, -3.3428106e-03,  3.5822727e-03],\n",
            "       [-5.0889268e-03,  5.6601865e-03, -5.0403416e-04, ...,\n",
            "         3.0882021e-03,  5.5813994e-03, -5.1668910e-03],\n",
            "       [ 8.5346308e-03,  4.8556891e-03, -3.4992457e-03, ...,\n",
            "        -1.0260696e-02, -7.1898331e-03,  6.2835421e-03],\n",
            "       ...,\n",
            "       [ 9.3769208e-03, -1.8392928e-05, -4.5328862e-03, ...,\n",
            "         2.1075029e-03, -8.6317528e-03,  5.9649483e-03],\n",
            "       [ 2.8624628e-03, -4.8496947e-03,  8.8547654e-03, ...,\n",
            "         3.1564054e-03, -2.0551691e-03, -9.7373379e-03],\n",
            "       [-6.5686442e-03, -8.8593718e-03,  1.5980047e-03, ...,\n",
            "         5.0250004e-04,  9.9263545e-03,  3.1817704e-03]], dtype=float32)>\n",
            "b1: <tf.Variable 'Variable:0' shape=(512, 1) dtype=float32, numpy=\n",
            "array([[-2.37572618e-04],\n",
            "       [ 1.02909952e-02],\n",
            "       [ 2.07447167e-02],\n",
            "       [ 3.44360742e-04],\n",
            "       [ 2.03380175e-02],\n",
            "       [-3.53663578e-04],\n",
            "       [ 9.74803045e-03],\n",
            "       [ 2.64350092e-03],\n",
            "       [ 2.83579750e-04],\n",
            "       [ 2.09764577e-02],\n",
            "       [ 7.72941159e-03],\n",
            "       [ 7.18200207e-03],\n",
            "       [ 4.50138561e-03],\n",
            "       [ 3.01600434e-03],\n",
            "       [ 7.98410212e-04],\n",
            "       [ 7.82929640e-03],\n",
            "       [ 1.38222873e-02],\n",
            "       [ 2.80591454e-02],\n",
            "       [ 8.46746936e-03],\n",
            "       [-2.95459060e-03],\n",
            "       [ 2.39912067e-02],\n",
            "       [ 4.11208719e-03],\n",
            "       [ 1.11764073e-02],\n",
            "       [ 9.67229623e-03],\n",
            "       [-8.28157738e-03],\n",
            "       [-5.75502496e-03],\n",
            "       [ 1.77466106e-02],\n",
            "       [ 1.84408817e-02],\n",
            "       [ 5.53690596e-03],\n",
            "       [-5.56934567e-04],\n",
            "       [ 7.89508875e-03],\n",
            "       [ 1.42303491e-02],\n",
            "       [ 1.04398998e-02],\n",
            "       [ 8.41910578e-03],\n",
            "       [ 8.14137794e-03],\n",
            "       [ 3.67740315e-04],\n",
            "       [-9.86868702e-03],\n",
            "       [ 1.27356844e-02],\n",
            "       [ 1.86599623e-02],\n",
            "       [-2.69584544e-03],\n",
            "       [-3.81685793e-03],\n",
            "       [ 2.45905202e-03],\n",
            "       [ 5.48993936e-03],\n",
            "       [ 1.19266631e-02],\n",
            "       [ 2.62506176e-02],\n",
            "       [ 5.42302034e-04],\n",
            "       [-2.24213954e-03],\n",
            "       [-2.10734503e-03],\n",
            "       [ 1.81083642e-02],\n",
            "       [ 1.81352608e-02],\n",
            "       [ 9.66626685e-03],\n",
            "       [-8.64533300e-04],\n",
            "       [-7.14341924e-03],\n",
            "       [ 1.35974595e-02],\n",
            "       [ 9.95446742e-03],\n",
            "       [ 5.88901981e-04],\n",
            "       [ 1.04446122e-02],\n",
            "       [ 1.19481497e-02],\n",
            "       [ 2.13179970e-04],\n",
            "       [-2.24973162e-04],\n",
            "       [ 3.62900086e-02],\n",
            "       [ 5.23249432e-03],\n",
            "       [ 1.83619708e-02],\n",
            "       [ 7.75872148e-04],\n",
            "       [ 7.97563605e-03],\n",
            "       [ 7.68830488e-03],\n",
            "       [ 1.25487719e-03],\n",
            "       [ 1.63792595e-02],\n",
            "       [ 1.22394355e-03],\n",
            "       [ 1.30365686e-02],\n",
            "       [-1.63638815e-02],\n",
            "       [ 2.05556918e-02],\n",
            "       [ 1.09442510e-02],\n",
            "       [ 2.69683395e-02],\n",
            "       [ 1.28064081e-02],\n",
            "       [ 2.05999217e-03],\n",
            "       [ 3.52233090e-02],\n",
            "       [ 1.60487695e-03],\n",
            "       [ 5.22450777e-03],\n",
            "       [-5.65496273e-03],\n",
            "       [-4.68964595e-03],\n",
            "       [ 6.69595739e-03],\n",
            "       [ 2.32484024e-02],\n",
            "       [ 4.10411693e-03],\n",
            "       [ 1.90582201e-02],\n",
            "       [ 9.62460320e-03],\n",
            "       [ 5.31324465e-03],\n",
            "       [ 3.44439381e-04],\n",
            "       [ 3.71460021e-02],\n",
            "       [ 1.58653234e-03],\n",
            "       [-1.07551804e-02],\n",
            "       [-2.90597149e-04],\n",
            "       [ 1.01340478e-02],\n",
            "       [-1.16823856e-02],\n",
            "       [ 1.25070401e-02],\n",
            "       [ 1.59891844e-02],\n",
            "       [ 2.80327685e-02],\n",
            "       [ 4.58057644e-03],\n",
            "       [ 2.68783909e-03],\n",
            "       [-7.66484663e-05],\n",
            "       [ 2.72056874e-04],\n",
            "       [-1.99682661e-03],\n",
            "       [-1.91060128e-04],\n",
            "       [ 3.57940444e-04],\n",
            "       [ 3.51733947e-03],\n",
            "       [ 4.26525343e-03],\n",
            "       [ 6.52992632e-03],\n",
            "       [ 2.44647439e-04],\n",
            "       [-7.37608830e-03],\n",
            "       [ 1.71024352e-03],\n",
            "       [ 4.90190387e-02],\n",
            "       [-1.98726542e-03],\n",
            "       [-1.45074807e-03],\n",
            "       [-1.33535499e-03],\n",
            "       [ 9.75880120e-03],\n",
            "       [ 2.88026780e-03],\n",
            "       [-2.50383571e-04],\n",
            "       [ 4.19551088e-03],\n",
            "       [ 2.70223105e-03],\n",
            "       [ 6.52892189e-03],\n",
            "       [ 6.21683104e-03],\n",
            "       [ 1.41784875e-02],\n",
            "       [ 1.36465114e-02],\n",
            "       [ 3.57652339e-03],\n",
            "       [ 1.93231888e-02],\n",
            "       [ 8.71731900e-03],\n",
            "       [ 4.55074850e-03],\n",
            "       [ 9.72993858e-03],\n",
            "       [-5.05939918e-03],\n",
            "       [ 8.49939138e-03],\n",
            "       [-1.30913043e-02],\n",
            "       [-5.27708698e-03],\n",
            "       [ 3.92738450e-03],\n",
            "       [-1.06606334e-02],\n",
            "       [-6.25890866e-03],\n",
            "       [ 8.44096392e-03],\n",
            "       [-2.44776660e-04],\n",
            "       [ 2.79378612e-03],\n",
            "       [ 1.14227580e-02],\n",
            "       [ 2.56956578e-03],\n",
            "       [ 1.22957258e-02],\n",
            "       [ 6.28979877e-03],\n",
            "       [ 6.68482529e-03],\n",
            "       [-5.13122790e-03],\n",
            "       [ 7.38784019e-03],\n",
            "       [ 5.41933987e-04],\n",
            "       [ 2.43909527e-02],\n",
            "       [ 5.26983058e-03],\n",
            "       [ 1.40032638e-03],\n",
            "       [ 1.15686364e-03],\n",
            "       [-8.09875410e-03],\n",
            "       [ 1.34625239e-02],\n",
            "       [-6.79936912e-03],\n",
            "       [ 2.70010321e-03],\n",
            "       [ 2.93282093e-03],\n",
            "       [ 4.70039854e-03],\n",
            "       [ 1.10560991e-02],\n",
            "       [ 1.50154973e-03],\n",
            "       [-7.52863986e-03],\n",
            "       [ 1.78699708e-03],\n",
            "       [-3.25053977e-03],\n",
            "       [ 1.28366686e-02],\n",
            "       [ 3.35526429e-02],\n",
            "       [ 3.57237621e-03],\n",
            "       [ 9.82436351e-03],\n",
            "       [ 2.37431657e-03],\n",
            "       [ 1.85939018e-03],\n",
            "       [-3.42469476e-03],\n",
            "       [ 8.90598446e-03],\n",
            "       [ 3.50224972e-03],\n",
            "       [ 1.63963810e-02],\n",
            "       [-2.99121253e-04],\n",
            "       [ 1.20788226e-02],\n",
            "       [ 1.38864145e-02],\n",
            "       [ 2.52388455e-02],\n",
            "       [ 2.67194631e-03],\n",
            "       [ 8.26890767e-03],\n",
            "       [ 4.74651717e-03],\n",
            "       [ 8.37664679e-03],\n",
            "       [ 1.12021612e-02],\n",
            "       [ 7.78456964e-03],\n",
            "       [ 1.48218859e-03],\n",
            "       [-4.33628960e-03],\n",
            "       [ 2.44772993e-02],\n",
            "       [ 1.39353136e-02],\n",
            "       [-9.94883711e-04],\n",
            "       [ 1.45583937e-04],\n",
            "       [ 5.56242245e-04],\n",
            "       [ 1.21681276e-03],\n",
            "       [-1.97977037e-03],\n",
            "       [ 8.74958280e-03],\n",
            "       [-1.82865333e-04],\n",
            "       [ 1.06096128e-03],\n",
            "       [ 5.29677258e-04],\n",
            "       [ 1.34717429e-03],\n",
            "       [-6.43971097e-03],\n",
            "       [ 3.85823217e-03],\n",
            "       [-3.29513871e-03],\n",
            "       [ 2.61487067e-02],\n",
            "       [ 5.14197629e-03],\n",
            "       [ 1.91428885e-02],\n",
            "       [ 1.71188042e-02],\n",
            "       [ 9.80622787e-03],\n",
            "       [ 2.67939479e-03],\n",
            "       [ 8.06819089e-03],\n",
            "       [-8.96182260e-04],\n",
            "       [-3.60246375e-03],\n",
            "       [-7.57963717e-05],\n",
            "       [ 3.24360933e-03],\n",
            "       [ 1.16061457e-02],\n",
            "       [-9.31602321e-04],\n",
            "       [ 2.02633883e-03],\n",
            "       [ 6.54828316e-03],\n",
            "       [ 1.25320992e-02],\n",
            "       [ 1.70544945e-02],\n",
            "       [ 2.06961529e-03],\n",
            "       [ 3.64645943e-03],\n",
            "       [ 8.11984297e-03],\n",
            "       [-1.18993630e-03],\n",
            "       [ 1.72536413e-03],\n",
            "       [-5.29310550e-04],\n",
            "       [-1.59965572e-03],\n",
            "       [ 1.32096540e-02],\n",
            "       [ 2.46997713e-03],\n",
            "       [ 4.00441140e-03],\n",
            "       [ 6.15870254e-03],\n",
            "       [ 4.21819364e-04],\n",
            "       [-4.33076918e-03],\n",
            "       [ 1.29370240e-03],\n",
            "       [ 1.03520951e-03],\n",
            "       [ 3.22230756e-02],\n",
            "       [ 3.77062312e-03],\n",
            "       [ 9.08670668e-03],\n",
            "       [-4.46087448e-03],\n",
            "       [-3.41102527e-03],\n",
            "       [ 4.86802263e-03],\n",
            "       [-2.01024828e-04],\n",
            "       [ 2.32055448e-02],\n",
            "       [ 7.84030836e-03],\n",
            "       [ 6.43304596e-03],\n",
            "       [ 8.39977898e-03],\n",
            "       [ 3.26284692e-02],\n",
            "       [ 4.57452470e-03],\n",
            "       [-2.33637709e-02],\n",
            "       [-3.61583487e-04],\n",
            "       [-5.31278132e-03],\n",
            "       [-6.46542199e-03],\n",
            "       [-1.60029512e-02],\n",
            "       [-4.90999315e-04],\n",
            "       [ 5.10922819e-03],\n",
            "       [ 8.97923484e-03],\n",
            "       [ 7.26016145e-03],\n",
            "       [ 2.95886979e-03],\n",
            "       [ 1.25318784e-02],\n",
            "       [ 2.38196063e-03],\n",
            "       [ 6.14920445e-03],\n",
            "       [ 3.32532101e-04],\n",
            "       [ 1.77687481e-02],\n",
            "       [-8.47925153e-03],\n",
            "       [ 1.52509182e-03],\n",
            "       [ 9.82658379e-03],\n",
            "       [ 1.92953937e-03],\n",
            "       [-6.59732264e-04],\n",
            "       [-1.52919616e-03],\n",
            "       [-3.24967870e-04],\n",
            "       [ 1.31174605e-02],\n",
            "       [-3.17312020e-04],\n",
            "       [ 6.00086804e-03],\n",
            "       [ 3.48607497e-03],\n",
            "       [ 8.18910426e-04],\n",
            "       [ 5.31099038e-03],\n",
            "       [ 8.16183351e-03],\n",
            "       [ 1.29527338e-02],\n",
            "       [ 1.34482328e-02],\n",
            "       [ 3.57058551e-03],\n",
            "       [ 3.49207665e-03],\n",
            "       [ 2.58720685e-02],\n",
            "       [ 3.45031125e-03],\n",
            "       [-2.42943037e-03],\n",
            "       [ 1.83109369e-03],\n",
            "       [ 2.39782757e-03],\n",
            "       [-1.15433680e-02],\n",
            "       [ 2.88242265e-03],\n",
            "       [ 4.26799199e-03],\n",
            "       [ 1.54221654e-02],\n",
            "       [ 2.29614880e-03],\n",
            "       [ 3.06767615e-04],\n",
            "       [ 3.24847251e-02],\n",
            "       [ 4.97752742e-04],\n",
            "       [ 1.62002817e-02],\n",
            "       [ 6.32302137e-03],\n",
            "       [ 5.41084300e-05],\n",
            "       [ 1.31222662e-02],\n",
            "       [ 2.89630075e-03],\n",
            "       [-2.76751281e-03],\n",
            "       [ 4.14142385e-03],\n",
            "       [-1.23804423e-03],\n",
            "       [-4.00091521e-03],\n",
            "       [ 5.71897440e-03],\n",
            "       [ 4.08973219e-03],\n",
            "       [ 1.42054437e-02],\n",
            "       [ 9.23611876e-03],\n",
            "       [-6.36407640e-04],\n",
            "       [ 3.58243147e-03],\n",
            "       [ 1.39387380e-02],\n",
            "       [-2.89430376e-03],\n",
            "       [ 2.13145255e-03],\n",
            "       [ 2.03855778e-03],\n",
            "       [ 2.21845638e-02],\n",
            "       [ 3.05893086e-03],\n",
            "       [-6.28004363e-03],\n",
            "       [ 1.37742376e-04],\n",
            "       [ 1.92144857e-04],\n",
            "       [ 7.25729798e-04],\n",
            "       [ 1.08562373e-02],\n",
            "       [-2.38381079e-04],\n",
            "       [ 2.70191394e-02],\n",
            "       [ 1.77492145e-02],\n",
            "       [ 1.34941598e-04],\n",
            "       [ 6.21533114e-03],\n",
            "       [ 1.56609376e-03],\n",
            "       [ 2.50378344e-03],\n",
            "       [ 1.44134543e-03],\n",
            "       [ 6.40504360e-02],\n",
            "       [ 6.38249051e-03],\n",
            "       [ 3.49951256e-03],\n",
            "       [ 7.42763732e-05],\n",
            "       [-1.38391962e-03],\n",
            "       [ 2.39822245e-03],\n",
            "       [ 7.81503564e-04],\n",
            "       [-1.67760570e-02],\n",
            "       [ 1.73942838e-02],\n",
            "       [-6.10303786e-03],\n",
            "       [ 6.25371886e-03],\n",
            "       [ 8.66268575e-03],\n",
            "       [ 1.74557418e-02],\n",
            "       [-6.52211718e-03],\n",
            "       [-2.78802169e-03],\n",
            "       [ 7.37389456e-03],\n",
            "       [ 4.84801456e-03],\n",
            "       [ 1.71530398e-03],\n",
            "       [ 8.86835530e-03],\n",
            "       [ 1.17105888e-02],\n",
            "       [ 2.36549182e-03],\n",
            "       [ 7.14178104e-03],\n",
            "       [ 5.22902934e-03],\n",
            "       [-3.01814661e-03],\n",
            "       [ 5.22827031e-03],\n",
            "       [ 5.12246648e-03],\n",
            "       [ 8.59538093e-03],\n",
            "       [ 7.72137474e-03],\n",
            "       [ 1.55621185e-03],\n",
            "       [ 5.56438390e-05],\n",
            "       [ 5.99865988e-03],\n",
            "       [ 9.45462380e-03],\n",
            "       [-2.60225404e-03],\n",
            "       [ 4.55749128e-03],\n",
            "       [ 2.89735128e-03],\n",
            "       [ 1.69124659e-02],\n",
            "       [ 8.25495925e-03],\n",
            "       [ 5.60819777e-03],\n",
            "       [ 5.93526941e-03],\n",
            "       [ 7.09340535e-03],\n",
            "       [-4.83707758e-03],\n",
            "       [ 1.67372951e-03],\n",
            "       [ 1.49927288e-02],\n",
            "       [ 1.92763621e-03],\n",
            "       [ 1.71195436e-02],\n",
            "       [ 1.36201866e-02],\n",
            "       [ 1.10122347e-02],\n",
            "       [-2.97036464e-03],\n",
            "       [ 9.03157284e-04],\n",
            "       [-1.24952057e-02],\n",
            "       [ 6.95334375e-03],\n",
            "       [-1.92595384e-04],\n",
            "       [ 4.35626041e-03],\n",
            "       [ 2.47614604e-04],\n",
            "       [ 3.55052180e-04],\n",
            "       [ 2.44788756e-03],\n",
            "       [ 9.73389542e-05],\n",
            "       [ 3.00099677e-03],\n",
            "       [ 3.76586267e-03],\n",
            "       [ 3.28333885e-03],\n",
            "       [ 1.94650479e-02],\n",
            "       [ 2.09485311e-02],\n",
            "       [ 3.61137884e-03],\n",
            "       [ 6.71526464e-03],\n",
            "       [ 1.56703417e-03],\n",
            "       [ 2.80305510e-03],\n",
            "       [ 1.32314360e-03],\n",
            "       [ 4.38505085e-03],\n",
            "       [-2.42652372e-02],\n",
            "       [ 6.22069603e-03],\n",
            "       [ 1.76960870e-03],\n",
            "       [ 1.25191519e-02],\n",
            "       [-1.08669396e-03],\n",
            "       [ 5.49359759e-03],\n",
            "       [ 6.29159948e-03],\n",
            "       [ 6.57697627e-03],\n",
            "       [-2.01740884e-03],\n",
            "       [ 9.32479836e-03],\n",
            "       [ 5.80495631e-04],\n",
            "       [ 9.32807568e-04],\n",
            "       [ 3.24767008e-02],\n",
            "       [ 5.68808336e-03],\n",
            "       [-4.33161249e-03],\n",
            "       [-2.49932869e-03],\n",
            "       [ 3.56030278e-03],\n",
            "       [ 9.74865165e-03],\n",
            "       [ 1.38961873e-03],\n",
            "       [-1.73188411e-02],\n",
            "       [-5.97898324e-04],\n",
            "       [ 2.58032785e-04],\n",
            "       [ 2.05403604e-02],\n",
            "       [-1.36548630e-03],\n",
            "       [ 2.41544307e-03],\n",
            "       [ 6.53991447e-05],\n",
            "       [ 9.68632940e-03],\n",
            "       [ 1.67791863e-04],\n",
            "       [ 3.46346851e-03],\n",
            "       [ 5.21165831e-03],\n",
            "       [ 1.48580326e-02],\n",
            "       [ 2.94000958e-04],\n",
            "       [ 1.52973842e-03],\n",
            "       [-1.47014938e-03],\n",
            "       [ 6.92789908e-04],\n",
            "       [ 4.12952993e-03],\n",
            "       [ 7.90549791e-04],\n",
            "       [ 5.66166406e-03],\n",
            "       [ 7.10018724e-03],\n",
            "       [ 1.62260383e-02],\n",
            "       [-2.70325341e-04],\n",
            "       [ 3.10650142e-03],\n",
            "       [ 8.68320186e-03],\n",
            "       [-1.96594838e-03],\n",
            "       [ 1.59983174e-03],\n",
            "       [ 3.00039141e-03],\n",
            "       [ 2.40046959e-02],\n",
            "       [ 3.88432620e-03],\n",
            "       [ 4.88152774e-03],\n",
            "       [ 1.29841513e-03],\n",
            "       [ 6.86077494e-03],\n",
            "       [ 2.17093760e-03],\n",
            "       [ 5.51501215e-02],\n",
            "       [-5.31947240e-03],\n",
            "       [ 4.51666303e-03],\n",
            "       [ 6.20225025e-03],\n",
            "       [ 1.68636777e-02],\n",
            "       [ 1.11792684e-02],\n",
            "       [-9.83463542e-05],\n",
            "       [ 1.06156990e-03],\n",
            "       [ 3.38460575e-03],\n",
            "       [ 7.24869687e-03],\n",
            "       [ 8.05172697e-03],\n",
            "       [ 6.67121448e-03],\n",
            "       [ 7.36001600e-03],\n",
            "       [ 7.16144638e-03],\n",
            "       [ 5.17946556e-02],\n",
            "       [ 2.67073559e-03],\n",
            "       [ 1.16384076e-02],\n",
            "       [ 9.57002805e-04],\n",
            "       [ 1.23071363e-02],\n",
            "       [ 7.07877427e-03],\n",
            "       [ 9.35924426e-03],\n",
            "       [ 6.67736121e-03],\n",
            "       [ 7.99170230e-03],\n",
            "       [ 1.55367739e-02],\n",
            "       [ 7.72804487e-03],\n",
            "       [ 1.57777127e-02],\n",
            "       [ 1.97220128e-04],\n",
            "       [ 1.73192732e-02],\n",
            "       [ 1.75952737e-03],\n",
            "       [ 1.95264153e-03],\n",
            "       [ 2.83504021e-03],\n",
            "       [ 3.14727612e-02],\n",
            "       [ 1.01682854e-05],\n",
            "       [ 1.41220894e-02],\n",
            "       [ 1.18564884e-03],\n",
            "       [ 2.27127224e-02],\n",
            "       [ 2.12125815e-02],\n",
            "       [ 1.25535335e-02],\n",
            "       [ 6.24352926e-03],\n",
            "       [ 6.53287536e-03],\n",
            "       [ 1.65218003e-02],\n",
            "       [-1.20557216e-03],\n",
            "       [ 2.90271919e-02],\n",
            "       [ 1.56568235e-03],\n",
            "       [ 6.46425877e-03],\n",
            "       [ 1.82993035e-03],\n",
            "       [ 2.66559310e-02],\n",
            "       [ 6.24591019e-04],\n",
            "       [ 4.75394679e-03],\n",
            "       [ 2.93350476e-03],\n",
            "       [ 6.15456421e-03],\n",
            "       [ 4.43502190e-03],\n",
            "       [-9.48272587e-04],\n",
            "       [ 3.83028033e-04],\n",
            "       [ 8.12911987e-03],\n",
            "       [-2.30608042e-03],\n",
            "       [ 8.09241552e-03],\n",
            "       [-8.89658113e-04],\n",
            "       [-5.40088629e-03],\n",
            "       [ 8.95434339e-03],\n",
            "       [ 5.61012048e-03],\n",
            "       [ 1.31493602e-02],\n",
            "       [ 3.08921374e-03],\n",
            "       [ 4.03863983e-03],\n",
            "       [-9.63775616e-04],\n",
            "       [ 2.83313747e-02],\n",
            "       [ 7.37971347e-03],\n",
            "       [ 2.91319215e-03],\n",
            "       [ 2.42094267e-02]], dtype=float32)>\n",
            "W2: <tf.Variable 'Variable:0' shape=(256, 512) dtype=float32, numpy=\n",
            "array([[-0.00180512,  0.00023782,  0.01250355, ..., -0.00667035,\n",
            "         0.00615845, -0.00258746],\n",
            "       [-0.01013837, -0.01054099, -0.00152475, ...,  0.01375996,\n",
            "        -0.00846638,  0.02451237],\n",
            "       [ 0.00501599,  0.05694263,  0.04481   , ...,  0.00545868,\n",
            "         0.01001579,  0.01483984],\n",
            "       ...,\n",
            "       [-0.00773922, -0.01055827,  0.00114996, ...,  0.00119848,\n",
            "        -0.0090801 ,  0.01688534],\n",
            "       [-0.00468194, -0.01968473,  0.00785121, ...,  0.01113724,\n",
            "         0.0051948 ,  0.00901381],\n",
            "       [ 0.00382223,  0.01098407,  0.01680223, ..., -0.0043794 ,\n",
            "        -0.00419347, -0.0035484 ]], dtype=float32)>\n",
            "b2: <tf.Variable 'Variable:0' shape=(256, 1) dtype=float32, numpy=\n",
            "array([[ 4.86835279e-03],\n",
            "       [ 3.22029181e-02],\n",
            "       [ 8.64292458e-02],\n",
            "       [ 3.22001893e-03],\n",
            "       [-1.31782639e-04],\n",
            "       [ 1.03042815e-02],\n",
            "       [-2.43981907e-04],\n",
            "       [ 2.34213378e-03],\n",
            "       [ 1.13533475e-02],\n",
            "       [-1.73605839e-03],\n",
            "       [ 1.10049238e-02],\n",
            "       [ 3.04338001e-02],\n",
            "       [ 1.64710321e-02],\n",
            "       [ 2.23544538e-02],\n",
            "       [-8.72455654e-04],\n",
            "       [ 1.42736742e-02],\n",
            "       [-1.54248881e-03],\n",
            "       [ 7.48986099e-03],\n",
            "       [ 3.08935605e-02],\n",
            "       [ 1.32189831e-02],\n",
            "       [ 3.91580135e-04],\n",
            "       [ 2.13228576e-02],\n",
            "       [ 3.17754708e-02],\n",
            "       [ 7.29309453e-04],\n",
            "       [ 3.39059788e-03],\n",
            "       [ 8.20130855e-03],\n",
            "       [ 3.97190405e-03],\n",
            "       [ 9.34860762e-03],\n",
            "       [ 3.55157480e-02],\n",
            "       [ 4.02975529e-02],\n",
            "       [-1.77780411e-03],\n",
            "       [ 1.79515760e-02],\n",
            "       [ 2.47466285e-02],\n",
            "       [-2.26530293e-03],\n",
            "       [ 5.63596375e-03],\n",
            "       [ 7.50504527e-03],\n",
            "       [ 2.15472337e-02],\n",
            "       [ 4.31946963e-02],\n",
            "       [ 4.74952022e-03],\n",
            "       [ 2.90632676e-02],\n",
            "       [ 1.91775151e-02],\n",
            "       [ 1.95884854e-02],\n",
            "       [-2.30064662e-03],\n",
            "       [ 3.68516818e-02],\n",
            "       [ 2.42853593e-02],\n",
            "       [ 7.31588714e-03],\n",
            "       [ 3.83766703e-02],\n",
            "       [ 2.15669181e-02],\n",
            "       [ 2.34667566e-02],\n",
            "       [ 6.08327426e-02],\n",
            "       [ 1.80540339e-03],\n",
            "       [ 3.03650144e-02],\n",
            "       [ 1.96297690e-02],\n",
            "       [ 7.48903397e-03],\n",
            "       [ 1.24876422e-03],\n",
            "       [-1.80341443e-03],\n",
            "       [ 1.23457182e-02],\n",
            "       [ 1.56381484e-02],\n",
            "       [ 1.97793916e-02],\n",
            "       [ 3.65418233e-02],\n",
            "       [ 1.78681761e-02],\n",
            "       [ 4.88249725e-03],\n",
            "       [ 2.02228948e-02],\n",
            "       [-1.45172421e-03],\n",
            "       [ 2.25507095e-02],\n",
            "       [ 7.57335685e-03],\n",
            "       [ 2.34336294e-02],\n",
            "       [ 9.94634721e-03],\n",
            "       [ 3.05980984e-02],\n",
            "       [ 3.37344296e-02],\n",
            "       [ 2.21334826e-02],\n",
            "       [ 1.82854896e-03],\n",
            "       [ 2.31197476e-02],\n",
            "       [ 1.08652702e-02],\n",
            "       [-2.89135077e-03],\n",
            "       [ 4.39232327e-02],\n",
            "       [ 6.71626255e-03],\n",
            "       [ 1.04904547e-02],\n",
            "       [-2.44273164e-04],\n",
            "       [ 1.32553922e-02],\n",
            "       [ 1.11820139e-02],\n",
            "       [ 3.76661681e-03],\n",
            "       [ 5.77765331e-03],\n",
            "       [ 3.26263569e-02],\n",
            "       [ 5.70847699e-03],\n",
            "       [ 3.82839218e-02],\n",
            "       [ 1.14913285e-02],\n",
            "       [ 4.50104102e-03],\n",
            "       [ 3.01408581e-03],\n",
            "       [ 1.30475089e-02],\n",
            "       [ 1.59578491e-02],\n",
            "       [ 1.77820735e-02],\n",
            "       [ 9.50065628e-03],\n",
            "       [ 3.57007831e-02],\n",
            "       [ 2.84511428e-02],\n",
            "       [-4.50956490e-04],\n",
            "       [-1.95992016e-03],\n",
            "       [ 8.51648860e-03],\n",
            "       [ 4.03613672e-02],\n",
            "       [ 9.86027997e-03],\n",
            "       [ 1.57853421e-02],\n",
            "       [ 2.96395626e-02],\n",
            "       [ 2.35894211e-02],\n",
            "       [-2.90353689e-03],\n",
            "       [ 1.13199772e-02],\n",
            "       [ 2.74364352e-02],\n",
            "       [ 2.61261296e-02],\n",
            "       [-1.52317487e-04],\n",
            "       [ 3.70714770e-05],\n",
            "       [ 1.38166696e-02],\n",
            "       [-2.03558989e-03],\n",
            "       [ 4.14664894e-02],\n",
            "       [ 2.74232440e-02],\n",
            "       [ 3.05117294e-02],\n",
            "       [ 7.84374587e-03],\n",
            "       [ 1.00886999e-02],\n",
            "       [ 1.09189991e-02],\n",
            "       [-2.21813726e-03],\n",
            "       [ 1.74386718e-04],\n",
            "       [ 4.28392850e-02],\n",
            "       [ 6.89923484e-03],\n",
            "       [ 2.61980686e-02],\n",
            "       [-2.11610342e-03],\n",
            "       [ 2.11355183e-02],\n",
            "       [ 1.18931346e-02],\n",
            "       [ 1.23341372e-02],\n",
            "       [ 6.37428137e-03],\n",
            "       [ 2.21228343e-04],\n",
            "       [ 9.73476749e-03],\n",
            "       [ 2.82473024e-03],\n",
            "       [-1.93255115e-03],\n",
            "       [ 6.12340607e-02],\n",
            "       [ 1.56134116e-02],\n",
            "       [-4.19470435e-03],\n",
            "       [ 1.61128752e-02],\n",
            "       [ 1.71978828e-02],\n",
            "       [ 1.73771565e-04],\n",
            "       [ 1.08261360e-02],\n",
            "       [-2.48107035e-03],\n",
            "       [ 7.19425604e-02],\n",
            "       [-2.51609855e-03],\n",
            "       [ 1.40409209e-02],\n",
            "       [ 1.70068015e-04],\n",
            "       [ 6.10544346e-03],\n",
            "       [ 1.24911987e-03],\n",
            "       [-1.31859822e-04],\n",
            "       [ 1.86736509e-02],\n",
            "       [ 3.04253157e-02],\n",
            "       [ 1.00832656e-02],\n",
            "       [ 1.25238085e-02],\n",
            "       [-7.73530512e-04],\n",
            "       [ 2.54113730e-02],\n",
            "       [ 1.95393376e-02],\n",
            "       [-1.90565261e-04],\n",
            "       [ 2.47234087e-02],\n",
            "       [ 9.52662158e-05],\n",
            "       [ 1.65120661e-02],\n",
            "       [ 9.63163190e-03],\n",
            "       [ 4.20716330e-02],\n",
            "       [ 1.83633175e-02],\n",
            "       [ 2.94697192e-02],\n",
            "       [ 9.98174585e-03],\n",
            "       [ 3.04337707e-03],\n",
            "       [ 3.12842545e-03],\n",
            "       [ 8.93215463e-03],\n",
            "       [ 7.70265842e-03],\n",
            "       [ 8.66744108e-03],\n",
            "       [ 1.94454715e-02],\n",
            "       [ 1.65670980e-02],\n",
            "       [ 9.09088366e-03],\n",
            "       [ 1.72567703e-02],\n",
            "       [ 6.26187684e-05],\n",
            "       [ 3.10600270e-02],\n",
            "       [-2.75516254e-03],\n",
            "       [ 1.18388860e-02],\n",
            "       [ 1.18955458e-02],\n",
            "       [ 3.27781704e-03],\n",
            "       [ 2.52287313e-02],\n",
            "       [ 3.41884559e-03],\n",
            "       [ 5.32851405e-02],\n",
            "       [ 1.82173774e-02],\n",
            "       [ 6.67611882e-03],\n",
            "       [-1.35237258e-03],\n",
            "       [ 7.07640033e-03],\n",
            "       [ 1.87460822e-03],\n",
            "       [-1.89479208e-04],\n",
            "       [ 1.95326447e-03],\n",
            "       [ 4.24381666e-04],\n",
            "       [ 1.88906528e-02],\n",
            "       [ 3.16038094e-02],\n",
            "       [ 3.73152494e-02],\n",
            "       [-8.43338436e-04],\n",
            "       [ 2.52895132e-02],\n",
            "       [ 3.69448273e-04],\n",
            "       [ 2.70062871e-03],\n",
            "       [ 1.47680771e-02],\n",
            "       [ 8.44612066e-03],\n",
            "       [ 1.58293713e-02],\n",
            "       [ 2.55914237e-02],\n",
            "       [-1.35633640e-03],\n",
            "       [ 2.48323171e-03],\n",
            "       [ 2.84717660e-02],\n",
            "       [-8.34715669e-04],\n",
            "       [ 1.16747797e-04],\n",
            "       [ 6.42186170e-03],\n",
            "       [ 1.60748139e-02],\n",
            "       [ 3.74052338e-02],\n",
            "       [ 4.61189859e-02],\n",
            "       [ 1.35957543e-02],\n",
            "       [ 4.85297367e-02],\n",
            "       [ 2.71419669e-03],\n",
            "       [ 1.26155289e-02],\n",
            "       [-1.10006111e-03],\n",
            "       [ 9.90241300e-03],\n",
            "       [ 1.03542209e-02],\n",
            "       [ 3.73156928e-02],\n",
            "       [-2.99724576e-04],\n",
            "       [ 3.13705988e-02],\n",
            "       [ 8.89480785e-02],\n",
            "       [ 4.12364316e-04],\n",
            "       [ 5.30991284e-03],\n",
            "       [-4.56869195e-04],\n",
            "       [-1.20901514e-03],\n",
            "       [ 1.99636538e-02],\n",
            "       [ 2.37424914e-02],\n",
            "       [ 2.78715752e-02],\n",
            "       [ 8.61437060e-03],\n",
            "       [ 2.89626475e-02],\n",
            "       [ 1.21650053e-02],\n",
            "       [ 4.59756330e-02],\n",
            "       [ 4.28866260e-02],\n",
            "       [ 9.60941613e-03],\n",
            "       [ 3.41697372e-02],\n",
            "       [ 7.90189393e-03],\n",
            "       [ 8.02790280e-03],\n",
            "       [-2.14739051e-03],\n",
            "       [-3.10912519e-03],\n",
            "       [ 1.13949773e-03],\n",
            "       [ 8.23096198e-05],\n",
            "       [ 2.36294349e-03],\n",
            "       [-7.19521049e-05],\n",
            "       [ 2.75392216e-02],\n",
            "       [ 3.09211668e-02],\n",
            "       [ 3.15414416e-03],\n",
            "       [ 3.27283032e-02],\n",
            "       [ 4.64627072e-02],\n",
            "       [ 1.52811753e-02],\n",
            "       [ 1.24364602e-03],\n",
            "       [-4.89222817e-04],\n",
            "       [ 6.32511638e-03],\n",
            "       [ 1.84844453e-02],\n",
            "       [-1.97436148e-03],\n",
            "       [ 1.62192632e-03],\n",
            "       [ 1.42358933e-02],\n",
            "       [ 6.41302112e-03],\n",
            "       [ 5.73009998e-03]], dtype=float32)>\n",
            "W3: <tf.Variable 'Variable:0' shape=(128, 256) dtype=float32, numpy=\n",
            "array([[-0.00024676,  0.08098914,  0.11878223, ...,  0.02799364,\n",
            "         0.03361654,  0.00593293],\n",
            "       [ 0.00477843,  0.05009837,  0.13636403, ...,  0.01929176,\n",
            "         0.01369205,  0.01421263],\n",
            "       [ 0.00381407, -0.00707493, -0.01213145, ..., -0.00704292,\n",
            "         0.01531269,  0.00831074],\n",
            "       ...,\n",
            "       [ 0.01007907, -0.01310165, -0.0012118 , ...,  0.00155741,\n",
            "        -0.00627302,  0.00163627],\n",
            "       [ 0.01041873,  0.0359413 ,  0.14913076, ...,  0.01139772,\n",
            "         0.00347744,  0.01354149],\n",
            "       [-0.00686012,  0.00087607,  0.00961642, ..., -0.01072664,\n",
            "         0.00478457,  0.00992379]], dtype=float32)>\n",
            "b3: <tf.Variable 'Variable:0' shape=(128, 1) dtype=float32, numpy=\n",
            "array([[ 2.29576737e-01],\n",
            "       [ 1.27622545e-01],\n",
            "       [-5.02859317e-02],\n",
            "       [ 2.11637281e-02],\n",
            "       [-4.02622041e-04],\n",
            "       [-6.68123830e-04],\n",
            "       [-1.24759823e-01],\n",
            "       [ 1.15300998e-01],\n",
            "       [ 8.77392367e-02],\n",
            "       [ 2.65894900e-03],\n",
            "       [ 3.95237794e-03],\n",
            "       [ 1.44112915e-01],\n",
            "       [-6.58739824e-04],\n",
            "       [-3.03064066e-04],\n",
            "       [-8.17745677e-05],\n",
            "       [ 3.36632401e-01],\n",
            "       [-6.32323464e-03],\n",
            "       [ 6.88039586e-02],\n",
            "       [-3.39456052e-02],\n",
            "       [ 1.43998992e-02],\n",
            "       [ 4.55862703e-03],\n",
            "       [-3.37070413e-02],\n",
            "       [-1.63060846e-04],\n",
            "       [ 2.13265270e-02],\n",
            "       [ 1.62391931e-01],\n",
            "       [-1.83718849e-03],\n",
            "       [ 1.67127222e-01],\n",
            "       [ 6.11906126e-03],\n",
            "       [ 2.53174268e-02],\n",
            "       [-8.91376985e-05],\n",
            "       [ 4.19081451e-04],\n",
            "       [ 9.42938030e-03],\n",
            "       [ 9.00315940e-02],\n",
            "       [-7.88627639e-02],\n",
            "       [-3.98587324e-02],\n",
            "       [ 9.35951918e-02],\n",
            "       [ 1.79249002e-03],\n",
            "       [ 2.32393322e-05],\n",
            "       [-2.84069870e-03],\n",
            "       [-2.89624254e-03],\n",
            "       [-1.91807386e-03],\n",
            "       [ 8.45042318e-02],\n",
            "       [-7.64294935e-04],\n",
            "       [ 8.99755768e-03],\n",
            "       [-7.86704477e-03],\n",
            "       [-7.96878536e-04],\n",
            "       [-1.09774352e-03],\n",
            "       [ 4.47271504e-02],\n",
            "       [-9.09613911e-04],\n",
            "       [ 1.51238090e-03],\n",
            "       [-3.67136225e-02],\n",
            "       [ 1.02601372e-01],\n",
            "       [-1.17463653e-03],\n",
            "       [ 3.79139669e-02],\n",
            "       [-1.09519688e-02],\n",
            "       [ 1.26776844e-01],\n",
            "       [-1.63485762e-03],\n",
            "       [-1.99355260e-02],\n",
            "       [ 1.01164408e-01],\n",
            "       [-4.67113778e-02],\n",
            "       [ 5.81078753e-02],\n",
            "       [ 5.94897605e-02],\n",
            "       [-1.27096213e-02],\n",
            "       [ 6.02899194e-02],\n",
            "       [-1.46634839e-02],\n",
            "       [-2.92204972e-02],\n",
            "       [ 6.68991506e-02],\n",
            "       [-3.90869332e-03],\n",
            "       [ 6.67109862e-02],\n",
            "       [ 2.17119306e-02],\n",
            "       [ 5.65603003e-03],\n",
            "       [-1.91555023e-02],\n",
            "       [ 9.20536369e-03],\n",
            "       [-4.77922522e-02],\n",
            "       [ 2.21119472e-03],\n",
            "       [ 4.01232019e-02],\n",
            "       [-1.04984280e-03],\n",
            "       [-1.45322215e-02],\n",
            "       [-3.19986790e-03],\n",
            "       [-2.90379440e-03],\n",
            "       [-2.75465846e-03],\n",
            "       [-4.92221770e-05],\n",
            "       [-6.03487045e-02],\n",
            "       [-3.29567748e-03],\n",
            "       [-2.20057592e-02],\n",
            "       [-5.64779676e-02],\n",
            "       [-1.42201548e-03],\n",
            "       [-1.01815406e-02],\n",
            "       [ 4.57964763e-02],\n",
            "       [-3.44865955e-03],\n",
            "       [-1.71880499e-02],\n",
            "       [ 6.49229214e-02],\n",
            "       [ 1.59178600e-01],\n",
            "       [-4.69678489e-05],\n",
            "       [ 1.20194331e-01],\n",
            "       [ 2.92527932e-03],\n",
            "       [-1.75378239e-03],\n",
            "       [-3.22378092e-02],\n",
            "       [-4.04193159e-03],\n",
            "       [-4.76981625e-02],\n",
            "       [ 1.17405213e-03],\n",
            "       [ 1.44983649e-01],\n",
            "       [-6.37359975e-04],\n",
            "       [-1.30474083e-02],\n",
            "       [ 1.52450711e-01],\n",
            "       [ 9.62485280e-03],\n",
            "       [ 7.44591281e-02],\n",
            "       [ 8.41927230e-02],\n",
            "       [ 1.25110494e-02],\n",
            "       [ 1.15129903e-01],\n",
            "       [-5.35595529e-02],\n",
            "       [-1.95518658e-02],\n",
            "       [ 9.13884863e-02],\n",
            "       [ 8.30300003e-02],\n",
            "       [ 5.63876238e-03],\n",
            "       [ 2.24735230e-01],\n",
            "       [-9.59750358e-03],\n",
            "       [-2.06749002e-03],\n",
            "       [ 7.25535024e-03],\n",
            "       [-2.89781019e-03],\n",
            "       [ 1.00480795e-01],\n",
            "       [-3.65458652e-02],\n",
            "       [ 9.27155390e-02],\n",
            "       [-2.79507571e-04],\n",
            "       [ 1.46988839e-01],\n",
            "       [-1.28084850e-02],\n",
            "       [ 1.95967659e-01],\n",
            "       [-1.11965267e-02]], dtype=float32)>\n",
            "W4: <tf.Variable 'Variable:0' shape=(10, 128) dtype=float32, numpy=\n",
            "array([[-0.12533681, -0.35426983,  0.07385863, ...,  0.00069342,\n",
            "        -0.32973495, -0.01681566],\n",
            "       [ 0.22762892,  0.19545166, -0.24076167, ..., -0.00855729,\n",
            "         0.24366517, -0.0046523 ],\n",
            "       [ 0.07427456,  0.13925985, -0.01101689, ..., -0.00816408,\n",
            "         0.09901355, -0.00118647],\n",
            "       ...,\n",
            "       [ 0.0733423 ,  0.15130506, -0.05309761, ...,  0.01728529,\n",
            "         0.17486149,  0.00097362],\n",
            "       [ 0.04851411,  0.05276469,  0.03906388, ..., -0.01879204,\n",
            "         0.02910294,  0.00699814],\n",
            "       [-0.16607893, -0.1413004 ,  0.06401143, ...,  0.03125621,\n",
            "        -0.06744304,  0.00641742]], dtype=float32)>\n",
            "b4: <tf.Variable 'Variable:0' shape=(10, 1) dtype=float32, numpy=\n",
            "array([[-0.19286305],\n",
            "       [ 0.5630101 ],\n",
            "       [-0.3301145 ],\n",
            "       [-0.4535616 ],\n",
            "       [ 1.0108043 ],\n",
            "       [-0.18810286],\n",
            "       [-0.43443924],\n",
            "       [-0.11392516],\n",
            "       [-0.5142104 ],\n",
            "       [ 0.65340805]], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration 40: Avg Train Loss: 0.0065611484460532665, Validation Loss: 0.49307766556739807, Learning Rate: 0.01\n",
        "Reducing learning rate to 0.005\n",
        "\n",
        "After this iteration, learning reduced after every 10 epochs."
      ],
      "metadata": {
        "id": "uC6ONVfZPor7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=125\n",
        "plt.plot(range(0,iterations),train_history,'b')\n",
        "plt.plot(range(0,iterations),val_history,'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "f1UsxGjTQBym",
        "outputId": "b434d1e2-6d03-48a4-d6a7-29d0ff5165af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Add Dropout Regularization (5pt)\n",
        "To add dropout regularization, you should modify the forward pass when applied to the training data and\n",
        "multiply the output of neurons in each layer (Z) by 1/𝑝* a random matrix of zeros and ones sampled from a\n",
        "Bernoulli distribution with probability 𝑝. Where (𝑝.=1- dropout_rate) is the probability of keeping a neuron. To\n",
        "get a random matrix of zeros and ones with the same shape as Z, you can first create a Bernoulli distribution\n",
        "with probability 𝑝 using tfp.distributions.Bernoulli then sample from this distribution.\n",
        "Note: The dropout should only be applied during training that is, it should only be applied to the training data.\n",
        "The dropout should NOT be applied at the time of inference that is, it should NOT be applied to the validation\n",
        "and test sets.\n",
        "Also note that the dropout should NOT be applied to the final layer because we do not want to drop any\n",
        "neuron in the final layer ( the final layer in a multi-class classification problem should have as many neurons\n",
        "as the number of output classes)\n",
        "Re-train the model with dropout regularization ( use a dropout rate between 0.2-0.5 ( that is a keep\n",
        "probability 𝑝 between 0.8-0.5). Answer the following question:\n",
        "• Does dropout regularization help reduce the gap between train and validation losses?"
      ],
      "metadata": {
        "id": "cGy67F78Bs9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note:\n",
        " The forward_pass_with dropout should only be applied to the training data  The dropout should NOT be applied to the validation and test sets.  Also dropout is supposed to reduce the gap between training and validation losses and not increase it."
      ],
      "metadata": {
        "id": "kY3R1N2k4qtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================THE END====================="
      ],
      "metadata": {
        "id": "Q2g9Jejx-cLa"
      }
    }
  ]
}